window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = {"version": "0.9.5", "fields": ["qualname", "fullname", "annotation", "default_value", "signature", "bases", "doc"], "ref": "fullname", "documentStore": {"docs": {"lstm_clm": {"fullname": "lstm_clm", "modulename": "lstm_clm", "kind": "module", "doc": "<p>Package for the CLM model utilities.</p>\n\n<p>Imports are in the submodules <code>clm</code>, <code>vocab</code> and, <code>callbacks</code>.</p>\n"}, "lstm_clm.callbacks": {"fullname": "lstm_clm.callbacks", "modulename": "lstm_clm.callbacks", "kind": "module", "doc": "<p>Custom TF Callbacks for metrics.</p>\n"}, "lstm_clm.callbacks.JSDCallback": {"fullname": "lstm_clm.callbacks.JSDCallback", "modulename": "lstm_clm.callbacks", "qualname": "JSDCallback", "kind": "class", "doc": "<p>Callback to calculate the Jensen-Shannon divergence.</p>\n\n<p>JSD is calculatet between the NLL of the samples\nand the NLL of the training and validation sets.\nIf <code>train</code> and <code>val</code> are not provided,\nthe callback can be used as a standalone metric\nwith a fixed dataset using <code>get_jsd</code>.</p>\n\n<h6 id=\"attributes\">Attributes:</h6>\n\n<ul>\n<li><strong>gen (GeneratorWrapper):</strong>  The generator.</li>\n<li><strong>train (_DatasetProto | None):</strong>  The training dataset if used as a callback.</li>\n<li><strong>val (_DatasetProto | None):</strong>  The validation dataset if used as a callback.</li>\n<li><strong>sample_size (int):</strong>  The sample size.</li>\n<li><strong>batch_size (int):</strong>  The batch size.</li>\n<li><strong>verbose (int):</strong>  Verbosity level.</li>\n</ul>\n", "bases": "keras.src.callbacks.Callback"}, "lstm_clm.callbacks.JSDCallback.__init__": {"fullname": "lstm_clm.callbacks.JSDCallback.__init__", "modulename": "lstm_clm.callbacks", "qualname": "JSDCallback.__init__", "kind": "function", "doc": "<p>Initialize the JSDCallback.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gen:</strong>  The generator.</li>\n<li><strong>train:</strong>  The training dataset. Defaults to None.</li>\n<li><strong>val:</strong>  The validation dataset. Defaults to None.</li>\n<li><strong>sample_size:</strong>  The sample size. Defaults to 10000.</li>\n<li><strong>batch_size:</strong>  The batch size. Defaults to 1024.</li>\n<li><strong>verbose:</strong>  Verbosity level. Defaults to 0.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">multinomial</span><span class=\"o\">.</span><span class=\"n\">MultinomialCLM</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">jsd</span><span class=\"o\">.</span><span class=\"n\">_DatasetProto</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">val</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">jsd</span><span class=\"o\">.</span><span class=\"n\">_DatasetProto</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sample_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span>)</span>"}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"fullname": "lstm_clm.callbacks.JSDCallback.get_jsd", "modulename": "lstm_clm.callbacks", "qualname": "JSDCallback.get_jsd", "kind": "function", "doc": "<p>Get Jensen-Shannon divergence between the provided sets.</p>\n\n<p>It uses the NLL of the samples and the NLL of the training and validation sets.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>samples:</strong>  The samples</li>\n<li><strong>preds:</strong>  The predictions</li>\n<li><strong>train:</strong>  The training set</li>\n<li><strong>val:</strong>  The validation set</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A tuple with the calculated Jensen-Shannon divergence,\n  and the NLLs of the training and validation sets, and the samples</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">preds</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"o\">+</span><span class=\"n\">ScalarType</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">val</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"o\">+</span><span class=\"n\">ScalarType</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">]],</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">]],</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">]]]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"fullname": "lstm_clm.callbacks.JSDCallback.on_epoch_end", "modulename": "lstm_clm.callbacks", "qualname": "JSDCallback.on_epoch_end", "kind": "function", "doc": "<p>Calculate the Jensen-Shannon divergence after every epoch.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>epoch:</strong>  The epoch number. Unused.</li>\n<li><strong>logs:</strong>  The logs. Defaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The logs</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">epoch</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">logs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span>:</span></span>", "funcdef": "def"}, "lstm_clm.callbacks.jensen_shannon_divergence": {"fullname": "lstm_clm.callbacks.jensen_shannon_divergence", "modulename": "lstm_clm.callbacks", "qualname": "jensen_shannon_divergence", "kind": "function", "doc": "<p>Calculates the Uniformity-completeness Jensen-Shannon divergence.</p>\n\n<h6 id=\"according-to\">According to:</h6>\n\n<blockquote>\n<pre><code>Ar\u00fas-Pous, J., Johansson, S.V., Prykhodko, O. et al.\nRandomized SMILES strings improve the quality of molecular generative models.\nJ Cheminform 11, 71 (2019).\n</code></pre>\n</blockquote>\n\n<ul>\n<li><a href=\"https://doi.org/10.1186/s13321-019-0393-0\">https://doi.org/10.1186/s13321-019-0393-0</a></li>\n<li>Implementation from <a href=\"https://github.com/undeadpixel/reinvent-randomized\">https://github.com/undeadpixel/reinvent-randomized</a></li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>*dists:</strong>  List of distributions to calc the divergence of\n(must be of the same length)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Calculated Jensen-Shannon divergence</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">dists</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm": {"fullname": "lstm_clm.clm", "modulename": "lstm_clm.clm", "kind": "module", "doc": "<p>Different classes to generate text from a lstm_clm.</p>\n"}, "lstm_clm.clm.BaseCLM": {"fullname": "lstm_clm.clm.BaseCLM", "modulename": "lstm_clm.clm", "qualname": "BaseCLM", "kind": "class", "doc": "<p>Absolute base class for all CLM models.</p>\n\n<p>Implements a default call per time step and a default sample function.\nCompliance with the <code>GenModel</code> protocol.</p>\n\n<h6 id=\"class-variables\">Class variables:</h6>\n\n<blockquote>\n  <ul>\n  <li>ASSERT_VOCAB (bool): Whether to assert that a vocabulary is provided.</li>\n  </ul>\n</blockquote>\n\n<h6 id=\"attributes\">Attributes:</h6>\n\n<ul>\n<li><strong>model (tf.keras.Model):</strong>  Model to wrap with LSTM layers.</li>\n<li><strong>vocab (Vocabulary | None):</strong>  Vocabulary.</li>\n<li><strong>vocab_size (int):</strong>  Size of vocabulary.</li>\n<li><strong>seq_len (int):</strong>  Sequence length.</li>\n<li><strong>dims (list[int]):</strong>  Number of neurons per LSTM layers.</li>\n<li><strong>has_embedding (bool):</strong>  Whether the model has an Embedding layer.</li>\n<li><strong>start_token (tf.Tensor):</strong>  Start token for generation.\nIf embedding: [1] {int32}\nElse: [1, Vocab] {float32}</li>\n</ul>\n"}, "lstm_clm.clm.BaseCLM.__init__": {"fullname": "lstm_clm.clm.BaseCLM.__init__", "modulename": "lstm_clm.clm", "qualname": "BaseCLM.__init__", "kind": "function", "doc": "<p>Initialize BaseCLM.</p>\n\n<p>The start index is the index of the BOS token in the vocabulary.\nThe sequence length append is 1 if the EOS token is present, else 0.\nThe values are extracted from the vocabulary, if provided.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model:</strong>  Model to wrap with LSTM layers.</li>\n<li><strong>max_len:</strong>  Maximum length of generated samples.</li>\n<li><strong>start_index:</strong>  Index of the start token. Defaults to 1.</li>\n<li><strong>seq_len_append:</strong>  Additional sequence length for EOS token. Defaults to 1.</li>\n<li><strong>vocab:</strong>  Vocabulary. Defaults to None.</li>\n</ul>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If vocabulary does not match provided lengths.</li>\n<li><strong>ValueError:</strong>  If <code>ASSERT_VOCAB</code> is True and no vocabulary is provided.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">engine</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">Model</span>,</span><span class=\"param\">\t<span class=\"n\">max_len</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">start_index</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">seq_len_append</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">vocab</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">VocabProto</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, "lstm_clm.clm.BaseCLM.from_vocab": {"fullname": "lstm_clm.clm.BaseCLM.from_vocab", "modulename": "lstm_clm.clm", "qualname": "BaseCLM.from_vocab", "kind": "function", "doc": "<p>Initialize <code>BaseCLM</code> from a vocabulary.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model:</strong>  Model to wrap with LSTM layers.</li>\n<li><strong>vocab:</strong>  Vocabulary.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p><code>BaseCLM</code> instance.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">engine</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">Model</span>,</span><span class=\"param\">\t<span class=\"n\">vocab</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">VocabProto</span></span><span class=\"return-annotation\">) -> <span class=\"n\">typing_extensions</span><span class=\"o\">.</span><span class=\"n\">Self</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.BaseCLM.call_cell": {"fullname": "lstm_clm.clm.BaseCLM.call_cell", "modulename": "lstm_clm.clm", "qualname": "BaseCLM.call_cell", "kind": "function", "doc": "<p>Call the model for one time step.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x_t:</strong>  Input at time step t.\nIf embedding: [Batch] {int32}\nElse: [Batch, Vocab] {float32}</li>\n<li><strong>hidden_states:</strong>  LSTM layers' hidden states.\n[2 x [Batch, Hidden] {float32}, ...]</li>\n<li><strong>training:</strong>  Whether in training mode. Defaults to False.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>tuple[tf.Tensor, list[list[tf.Tensor]], None]:\n      output: Time step t output. [Batch, Vocab] {float32}\n      states: LSTM layers' output states. [2 x [Batch, Hidden] {float32}, ...]\n      info: Always None.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">x_t</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_states</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"o\">/</span>,</span><span class=\"param\">\t<span class=\"n\">training</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]],</span> <span class=\"kc\">None</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.BaseCLM.load_optim": {"fullname": "lstm_clm.clm.BaseCLM.load_optim", "modulename": "lstm_clm.clm", "qualname": "BaseCLM.load_optim", "kind": "function", "doc": "<p>Load optimizer from a file and compile the model.</p>\n\n<h6 id=\"important\">Important:</h6>\n\n<blockquote>\n  <p>If batch normalization should be freezed,\n  use it before saving or loading the optimizer.</p>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Path to the file.</li>\n<li><strong>loss:</strong>  Loss function.</li>\n<li><strong>shape:</strong>  Shape of the in- and output of the model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">loss</span><span class=\"p\">:</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">losses</span><span class=\"o\">.</span><span class=\"n\">Loss</span> <span class=\"o\">|</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">shape</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.BaseCLM.save_optim": {"fullname": "lstm_clm.clm.BaseCLM.save_optim", "modulename": "lstm_clm.clm", "qualname": "BaseCLM.save_optim", "kind": "function", "doc": "<p>Save optimizer to a file.</p>\n\n<h6 id=\"important\">Important:</h6>\n\n<blockquote>\n  <p>use <code>freeze_batch_norm</code> before saving or loading the optimizer.</p>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  Path to the file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.MultinomialCLM": {"fullname": "lstm_clm.clm.MultinomialCLM", "modulename": "lstm_clm.clm", "qualname": "MultinomialCLM", "kind": "class", "doc": "<p>Base class for models generating samples with multinomial sampling.</p>\n\n<h6 id=\"attributes\">Attributes:</h6>\n\n<ul>\n<li><strong>model (tf.keras.Model):</strong>  Model to wrap with LSTM layers.</li>\n<li><strong>vocab (Vocabulary | None):</strong>  Vocabulary.</li>\n<li><strong>seq_len (int):</strong>  Sequence length.</li>\n<li><strong>dims (list[int]):</strong>  Number of neurons per LSTM layers.</li>\n<li><strong>has_embedding (bool):</strong>  Whether the model has an Embedding layer.</li>\n<li><strong>start_token (tf.Tensor):</strong>  Start token for generation.</li>\n</ul>\n", "bases": "lstm_clm.clm.base.BaseCLM"}, "lstm_clm.clm.MultinomialCLM.generate": {"fullname": "lstm_clm.clm.MultinomialCLM.generate", "modulename": "lstm_clm.clm", "qualname": "MultinomialCLM.generate", "kind": "function", "doc": "<p>Generate multiple batches of samples (wrapped in a tf.function).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>n_batches:</strong>  Number of batches to generate.</li>\n<li><strong>temp:</strong>  Temperature factor. Defaults to 1.0.</li>\n<li><strong>batch_size:</strong>  Batch size for generation. Defaults to 1024.</li>\n<li><strong>call_func:</strong>  Function to call model for one time step.\nDefaults to instance's <code>call_cell</code> method.</li>\n<li><strong>sample_func:</strong>  Function to sample next tokens.\nDefaults to instance's <code>sample_next_tokens</code> method.</li>\n<li><strong>verbose:</strong>  Verbosity level. Unused. Defaults to 0.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tuple with generated samples [Batches<em>Batch, Length] {int32}\n  and predictions [Batches</em>Batch, Length, Vocab] {float32}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">n_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">temp</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span>,</span><span class=\"param\">\t<span class=\"n\">call_func</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">CallFunc</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">sample_func</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">SampleFunc</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"fullname": "lstm_clm.clm.MultinomialCLM.sample_next_tokens", "modulename": "lstm_clm.clm", "qualname": "MultinomialCLM.sample_next_tokens", "kind": "function", "doc": "<p>Multinomially sample next tokens from the output (wrapped in a tf.function).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>i:</strong>  Current time step. [1] {int32}.</li>\n<li><strong>x_t:</strong>  Model output [Batch, Vocab] {float32}.</li>\n<li><strong>temp:</strong>  Temperature factor. [1]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Sampled next tokens [Batch] {int32}.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">x_t</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"o\">/</span>,</span><span class=\"param\">\t<span class=\"n\">temp</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"fullname": "lstm_clm.clm.MultinomialCLM.generate_with_perplexity", "modulename": "lstm_clm.clm", "qualname": "MultinomialCLM.generate_with_perplexity", "kind": "function", "doc": "<p>Generate samples with perplexity calculation from a model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>n_batches:</strong>  Number of batches to generate. Defaults to 1.</li>\n<li><strong>temp:</strong>  Temperature factor. Defaults to 1.0.</li>\n<li><strong>batch_size:</strong>  Batch size for generation. Defaults to 1024.</li>\n<li><strong>prior_probs:</strong>  Prior probabilities for each token.\nDefaults to equal probabilities for each non-special token.</li>\n<li><strong>verbose:</strong>  Verbosity level. Defaults to 0.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tuple with generated samples [Batches<em>Batch, Length] {int32} and\n  predictions [Batches</em>Batch, Length, Vocab] {float32} and\n  perplexity scores [Batches*Batch] {float32}</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If vocabulary is not provided.\nSet on instance creation or set <code>vocab</code> attribute.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">n_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">temp</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span>,</span><span class=\"param\">\t<span class=\"n\">prior_probs</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">lookup_ops</span><span class=\"o\">.</span><span class=\"n\">StaticHashTable</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.Trainer": {"fullname": "lstm_clm.clm.Trainer", "modulename": "lstm_clm.clm", "qualname": "Trainer", "kind": "class", "doc": "<p>Convenience class to train a model with a <code>RandomizedDataset</code>.</p>\n"}, "lstm_clm.clm.Trainer.fit": {"fullname": "lstm_clm.clm.Trainer.fit", "modulename": "lstm_clm.clm", "qualname": "Trainer.fit", "kind": "function", "doc": "<p>Fit the model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model:</strong>  The model</li>\n<li><strong>train:</strong>  The training dataset</li>\n<li><strong>val:</strong>  The validation dataset</li>\n<li><strong>epochs:</strong>  The number of epochs</li>\n<li><strong>batch_size:</strong>  The batch size</li>\n<li><strong>callbacks:</strong>  The list of callbacks</li>\n<li><strong>strategy:</strong>  The strategy. Defaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The history of the training</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">engine</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">Model</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">randomized</span><span class=\"o\">.</span><span class=\"n\">RandomizedDataset</span>,</span><span class=\"param\">\t<span class=\"n\">val</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">randomized</span><span class=\"o\">.</span><span class=\"n\">RandomizedDataset</span>,</span><span class=\"param\">\t<span class=\"n\">epochs</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">strategy</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">distribute</span><span class=\"o\">.</span><span class=\"n\">distribute_lib</span><span class=\"o\">.</span><span class=\"n\">Strategy</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">History</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.Trainer.callbacks": {"fullname": "lstm_clm.clm.Trainer.callbacks", "modulename": "lstm_clm.clm", "qualname": "Trainer.callbacks", "kind": "function", "doc": "<p>Get the callbacks for training.</p>\n\n<ul>\n<li>Additional positions:\n<ul>\n<li>0 at beginning,</li>\n<li>1 after JSD,</li>\n<li>2 after decay,</li>\n<li>3 after checkpoint,</li>\n<li>4 after trainingCP,</li>\n<li>5 after early stopping</li>\n</ul></li>\n<li>TODO: make the int as enum for add_cbs.</li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>clm:</strong>  The CLM</li>\n<li><strong>train:</strong>  The training dataset</li>\n<li><strong>val:</strong>  The validation dataset</li>\n<li><strong>cp:</strong>  The checkpoint to load. Defaults to None.</li>\n<li><strong>calc_jsd:</strong>  Whether to calc the JSD. Defaults to False.</li>\n<li><strong>decay:</strong>  The decay storer. Defaults to None.</li>\n<li><strong>models_path:</strong>  The _path to store the models. Defaults to None.</li>\n<li><strong>early_stopping:</strong>  The early stopping. Defaults to None.</li>\n<li><strong>additional_cbs:</strong>  Additional callbacks to add at specific positions.\nDefaults to empty dict.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The list of callbacks</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">clm</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">multinomial</span><span class=\"o\">.</span><span class=\"n\">MultinomialCLM</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">randomized</span><span class=\"o\">.</span><span class=\"n\">RandomizedDataset</span>,</span><span class=\"param\">\t<span class=\"n\">val</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">randomized</span><span class=\"o\">.</span><span class=\"n\">RandomizedDataset</span>,</span><span class=\"param\">\t<span class=\"n\">cp</span><span class=\"p\">:</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">ModelCheckpoint</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">calc_jsd</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">decay</span><span class=\"p\">:</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">ReduceLROnPlateau</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">models_path</span><span class=\"p\">:</span> <span class=\"n\">pathlib</span><span class=\"o\">.</span><span class=\"n\">Path</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping</span><span class=\"p\">:</span> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">EarlyStopping</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">additional_cbs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.Trainer.init": {"fullname": "lstm_clm.clm.Trainer.init", "modulename": "lstm_clm.clm", "qualname": "Trainer.init", "kind": "function", "doc": "<p>Initializes the model and strategy.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>build_model:</strong>  Function to build the model</li>\n<li><strong>build_opt:</strong>  Function to build the optimizer</li>\n<li><strong>multi_gpu:</strong>  Whether to use multiple GPUs. Defaults to False.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A tuple with the model, strategy, initial epoch, and decay storer/None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">build_model</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">Factory</span><span class=\"p\">[</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">engine</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">build_opt</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">Factory</span><span class=\"p\">[</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">engine</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">multi_gpu</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">engine</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">Model</span><span class=\"p\">,</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">StrategyProto</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.Trainer.build": {"fullname": "lstm_clm.clm.Trainer.build", "modulename": "lstm_clm.clm", "qualname": "Trainer.build", "kind": "function", "doc": "<p>Builds the train and val _datasets for training.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>train:</strong>  The training data.</li>\n<li><strong>val:</strong>  The validation data.</li>\n<li><strong>vocab:</strong>  The vocabulary.</li>\n<li><strong>rnd_func:</strong>  Function to randomize the SMILES strings.</li>\n<li><strong>epochs:</strong>  The number of epochs.</li>\n<li><strong>n_jobs:</strong>  The number of jobs.</li>\n<li><strong>just_shuffle:</strong>  Whether to just shuffle the data. Defaults to False.</li>\n<li><strong>init_epoch:</strong>  The initial epoch. Defaults to 0.</li>\n<li><strong>finetune:</strong>  Whether its finetune data and thus\ndata is precalc'd. Defaults to False.\nDefaults to 4.</li>\n<li><strong>verbose:</strong>  Verbosity level. Defaults to 0.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A tuple with train and val _datasets</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">vocab</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">VocabProto</span>,</span><span class=\"param\">\t<span class=\"n\">rnd_func</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">RandomizeFunc</span>,</span><span class=\"param\">\t<span class=\"n\">epochs</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_jobs</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">just_shuffle</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">init_epoch</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">finetune</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">randomized</span><span class=\"o\">.</span><span class=\"n\">RandomizedDataset</span><span class=\"p\">,</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">randomized</span><span class=\"o\">.</span><span class=\"n\">RandomizedDataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.VocabMultinomialCLM": {"fullname": "lstm_clm.clm.VocabMultinomialCLM", "modulename": "lstm_clm.clm", "qualname": "VocabMultinomialCLM", "kind": "class", "doc": "<p>Generating samples with multinomial sampling with vocabulary.</p>\n\n<h6 id=\"attributes\">Attributes:</h6>\n\n<ul>\n<li><strong>model (tf.keras.Model):</strong>  Model to wrap with LSTM layers.</li>\n<li><strong>vocab (Vocabulary):</strong>  Vocabulary.</li>\n<li><strong>seq_len (int):</strong>  Sequence length.</li>\n<li><strong>dims (list[int]):</strong>  Number of neurons per LSTM layers.</li>\n<li><strong>has_embedding (bool):</strong>  Whether the model has an Embedding layer.</li>\n<li><strong>start_token (tf.Tensor):</strong>  Start token for generation.</li>\n</ul>\n", "bases": "lstm_clm.clm.multinomial.MultinomialCLM"}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"fullname": "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB", "modulename": "lstm_clm.clm", "qualname": "VocabMultinomialCLM.ASSERT_VOCAB", "kind": "variable", "doc": "<p>Needs to be initialized with a vocabulary.</p>\n", "default_value": "True"}, "lstm_clm.clm.allow_memory_growth": {"fullname": "lstm_clm.clm.allow_memory_growth", "modulename": "lstm_clm.clm", "qualname": "allow_memory_growth", "kind": "function", "doc": "<p>Allow memory growth for TensorFlow GPU devices.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.batch_tensor_slices": {"fullname": "lstm_clm.clm.batch_tensor_slices", "modulename": "lstm_clm.clm", "qualname": "batch_tensor_slices", "kind": "function", "doc": "<p>Batch tensor slices to <code>tf.data.Dataset</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>data:</strong>  Data to batch. Either a single tensor or a tuple! of tensors.</li>\n<li><strong>batch_size:</strong>  Batch size.</li>\n<li><strong>desc:</strong>  Description for tqdm. Defaults to None.</li>\n<li><strong>verbose:</strong>  Verbosity level. Defaults to 0.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A iterable over a <code>tf.data.Dataset</code> of batched data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"o\">~</span><span class=\"n\">BatchableT</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">desc</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Iterable</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">BatchableT</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.build_adam_optimizer": {"fullname": "lstm_clm.clm.build_adam_optimizer", "modulename": "lstm_clm.clm", "qualname": "build_adam_optimizer", "kind": "function", "doc": "<p>Return an Adam optimizer with the given learning rate.</p>\n\n<p>As there are some issues with the Adam optimizer on Apple M1,\nwe use the legacy version in this case.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>learning_rate:</strong>  Initial learning rate.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Adam optimizer.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">learning_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.build_model_cp": {"fullname": "lstm_clm.clm.build_model_cp", "modulename": "lstm_clm.clm", "qualname": "build_model_cp", "kind": "function", "doc": "<p>Get the model checkpoint callback.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>models_path:</strong>  Path to save the models</li>\n<li><strong>scheme:</strong>  Scheme to save the models. Defaults to \"{epoch}.h5\".</li>\n<li><strong>best_only:</strong>  Save only the best model. Defaults to False.</li>\n<li><strong>weights_only:</strong>  Save only the weights. Defaults to True.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The model checkpoint callback</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">models_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">scheme</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;</span><span class=\"si\">{epoch}</span><span class=\"s1\">.h5&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">best_only</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">weights_only</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">ModelCheckpoint</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.get_data_from_vocab": {"fullname": "lstm_clm.clm.get_data_from_vocab", "modulename": "lstm_clm.clm", "qualname": "get_data_from_vocab", "kind": "function", "doc": "<p>Get the settings stored in a vocabulary.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>vocab:</strong>  Vocabulary.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A tuple with the vocabulary size (length of the tokens),\n  defined maximum length,\n  start index (index of the BOS token),\n  and sequence length append (1 if EOS token is present, else 0).</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">vocab</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">VocabProto</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.perplexity": {"fullname": "lstm_clm.clm.perplexity", "modulename": "lstm_clm.clm.perplexity", "kind": "module", "doc": "<p>Perplexity calculation for a lstm_clm.</p>\n"}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"fullname": "lstm_clm.clm.perplexity.build_default_prior_probs", "modulename": "lstm_clm.clm.perplexity", "qualname": "build_default_prior_probs", "kind": "function", "doc": "<p>Build a static hash table with prior probabilities for the vocabulary.</p>\n\n<p>Assigns equal prior probabilities to all characters\nin the vocabulary except for special tokens.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">vocab</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">special_tokens</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">lookup_ops</span><span class=\"o\">.</span><span class=\"n\">StaticHashTable</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.perplexity.build_hash_table": {"fullname": "lstm_clm.clm.perplexity.build_hash_table", "modulename": "lstm_clm.clm.perplexity", "qualname": "build_hash_table", "kind": "function", "doc": "<p>Build a static hash table with prior probabilities for the vocabulary.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>prior_probs:</strong>  Prior probabilities.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Static hash table with prior probabilities.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">prior_probs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">lookup_ops</span><span class=\"o\">.</span><span class=\"n\">StaticHashTable</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.perplexity.calc_perplexity": {"fullname": "lstm_clm.clm.perplexity.calc_perplexity", "modulename": "lstm_clm.clm.perplexity", "qualname": "calc_perplexity", "kind": "function", "doc": "<p>Calculate perplexity from samples and predictions (wrapped in a tf.function).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>samples:</strong>  Samples shifted (y_true). [Batch, Length] {int32}</li>\n<li><strong>preds:</strong>  Predictions. [Batch, Length[1:], Vocab] {float32}</li>\n<li><strong>weights:</strong>  Prior probabilities. [Batch, Length[1:]] {float32}</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Calculated Perplexity scores. [Batch] {float32}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">preds</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">weights</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"fullname": "lstm_clm.clm.perplexity.generate_with_perplexity", "modulename": "lstm_clm.clm.perplexity", "qualname": "generate_with_perplexity", "kind": "function", "doc": "<p>Generate samples with perplexity calculation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model:</strong>  Model to generate samples from.</li>\n<li><strong>n_batches:</strong>  Number of batches to generate.</li>\n<li><strong>temp:</strong>  Temperature factor. Defaults to 1.0.</li>\n<li><strong>batch_size:</strong>  Batch size. Defaults to 1024.</li>\n<li><strong>prior_probs:</strong>  Vocabulary prior probabilities.\nRequired if model is not VocabMultinomialCLM.</li>\n<li><strong>verbose:</strong>  Verbosity level. Defaults to 0.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n      samples: Generated samples. [Batch, Length] {int32}\n      preds: Predictions. [Batch, Length, Vocab] {float32}\n      perplexity: Perplexity scores. [Batch] {float32}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">MultinomialModel</span> <span class=\"o\">|</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">VocabMultinomialModel</span>,</span><span class=\"param\">\t<span class=\"n\">n_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">temp</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span>,</span><span class=\"param\">\t<span class=\"n\">prior_probs</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">lookup_ops</span><span class=\"o\">.</span><span class=\"n\">StaticHashTable</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"fullname": "lstm_clm.clm.perplexity.perplexity_from_samples", "modulename": "lstm_clm.clm.perplexity", "qualname": "perplexity_from_samples", "kind": "function", "doc": "<p>Calculate perplexity of samples.</p>\n\n<p>Note: samples (max_len + 2), preds (max_len + 1).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model:</strong>  Model to calculate perplexity from.</li>\n<li><strong>batch_size:</strong>  Batch size.</li>\n<li><strong>samples:</strong>  Previously generated samples. [Batch, Length] {int32}</li>\n<li><strong>preds:</strong>  Predictions. [Batch, Length[1:], Vocab] {float32}</li>\n<li><strong>prior_probs:</strong>  Vocabulary prior probabilities.\nDefaults to equal probabilities for non-special tokens.</li>\n<li><strong>verbose:</strong>  Verbosity level. Defaults to 0.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Perplexity scores. [Batch] {float32}</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>TypeError:</strong>  If prior_probs is not provided\nand model is not a VocabMultinomialModel.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">MultinomialModel</span> <span class=\"o\">|</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">VocabMultinomialModel</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">samples</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"o\">+</span><span class=\"n\">ScalarType</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">preds</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"o\">+</span><span class=\"n\">ScalarType</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prior_probs</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">lookup_ops</span><span class=\"o\">.</span><span class=\"n\">StaticHashTable</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.randomized": {"fullname": "lstm_clm.clm.randomized", "modulename": "lstm_clm.clm.randomized", "kind": "module", "doc": "<p>Utilities using randomized SMILES strings.</p>\n"}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"fullname": "lstm_clm.clm.randomized.BuiltRandomizedDataset", "modulename": "lstm_clm.clm.randomized", "qualname": "BuiltRandomizedDataset", "kind": "class", "doc": "<p>Batched and distributed dataset for training.</p>\n\n<h6 id=\"warns\">Warns:</h6>\n\n<blockquote>\n  <p>Built <code>RandomizedDataset</code> is infinite.\n  It will never stop yielding SMILES strings. Use with <code>steps_per_epoch</code>.</p>\n</blockquote>\n\n<h6 id=\"attributes\">Attributes:</h6>\n\n<ul>\n<li><strong>dataset (tf.data.Dataset | tf.distribute.DistributedDataset):</strong>  Yields <code>batch_size</code>-sized batches of randomized SMILES strings.</li>\n<li><strong>batch_size (int):</strong>  The batch size.</li>\n<li><strong>steps (int):</strong>  The number of steps per epoch.</li>\n<li><strong>init_epoch (int):</strong>  The initial epoch.</li>\n</ul>\n"}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"fullname": "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__", "modulename": "lstm_clm.clm.randomized", "qualname": "BuiltRandomizedDataset.__init__", "kind": "function", "doc": "<p>Creates a <code>tf.data.Dataset</code> from the <code>RandomizedDataset</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>ds:</strong>  The RandomizedDataset</li>\n<li><strong>batch_size:</strong>  The batch size. Defaults to 128.</li>\n<li><strong>strategy:</strong>  The strategy. Defaults to None.</li>\n<li><strong>init_epoch:</strong>  The initial epoch. Defaults to 0.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ds</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">randomized</span><span class=\"o\">.</span><span class=\"n\">RandomizedDataset</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>,</span><span class=\"param\">\t<span class=\"n\">strategy</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">distribute</span><span class=\"o\">.</span><span class=\"n\">distribute_lib</span><span class=\"o\">.</span><span class=\"n\">Strategy</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">init_epoch</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span>)</span>"}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset": {"fullname": "lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset", "modulename": "lstm_clm.clm.randomized", "qualname": "BuiltRandomizedDataset.dataset", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"fullname": "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size", "modulename": "lstm_clm.clm.randomized", "qualname": "BuiltRandomizedDataset.batch_size", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"fullname": "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch", "modulename": "lstm_clm.clm.randomized", "qualname": "BuiltRandomizedDataset.init_epoch", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.steps": {"fullname": "lstm_clm.clm.randomized.BuiltRandomizedDataset.steps", "modulename": "lstm_clm.clm.randomized", "qualname": "BuiltRandomizedDataset.steps", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.RandomizedDataset": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset", "kind": "class", "doc": "<p>Creates a dataset to randomized SMILES strings.</p>\n\n<h6 id=\"usage\">Usage:</h6>\n\n<blockquote>\n  <div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">mock_rnd_func</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">smis</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"n\">smis</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">smis</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">&quot;CCO&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;CCC&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;CCCC&quot;</span><span class=\"p\">]</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">vocabulary</span> <span class=\"o\">=</span> <span class=\"n\">Vocabulary</span><span class=\"p\">([</span><span class=\"s2\">&quot;C&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;O&quot;</span><span class=\"p\">]),</span> <span class=\"mi\">140</span><span class=\"p\">,</span> <span class=\"s2\">&quot;A&quot;</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">rd</span> <span class=\"o\">=</span> <span class=\"n\">RandomizedDataset</span><span class=\"p\">(</span><span class=\"n\">smis</span><span class=\"p\">,</span> <span class=\"n\">vocab</span><span class=\"p\">,</span> <span class=\"n\">mock_rnd_func</span><span class=\"p\">,</span> <span class=\"n\">n_jobs</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"go\">    rdd = rd.dataset(batch_size=1)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">rdd</span><span class=\"p\">,</span> <span class=\"n\">steps_per_epoch</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">rdd</span><span class=\"p\">),</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n</code></pre>\n  </div>\n</blockquote>\n\n<h6 id=\"attributes\">Attributes:</h6>\n\n<ul>\n<li><strong>smis (list[str]):</strong>  The SMILES strings to randomize.</li>\n<li><strong>vocab (VocabProto):</strong>  The vocabulary to use for encoding.</li>\n<li><strong>rnd_func (RandomizeFunc):</strong>  The function to use for randomizing SMILES strings.</li>\n<li><strong>n_jobs (int):</strong>  The number of jobs to use for parallelization</li>\n<li><strong>epoch (int):</strong>  The current epoch.</li>\n<li><strong>just_shuffle (bool):</strong>  If True, only shuffles the SMILES strings (only canonical).</li>\n<li><strong>n_precalc (int | None):</strong>  If not None, precalcs the randomized SMILES\nfor the first <code>n_precalc</code> epochs.</li>\n<li><strong>verbose (int):</strong>  Whether to show progress bars.</li>\n</ul>\n"}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.__init__", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.__init__", "kind": "function", "doc": "<p>Initialize the RandomizedDataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>smis:</strong>  The SMILES strings to randomize.</li>\n<li><strong>vocab:</strong>  The vocabulary to use for encoding.</li>\n<li><strong>rnd_func:</strong>  The function to use for randomizing SMILES strings.</li>\n<li><strong>n_jobs:</strong>  The number of jobs to use for parallelization.</li>\n<li><strong>init_epoch:</strong>  The initial epoch. Defaults to 0.</li>\n<li><strong>just_shuffle:</strong>  If True, only shuffles the SMILES strings (only canonical).\nDefaults to False.</li>\n<li><strong>n_precalc:</strong> If not None, precalcs the randomized SMILES\nfor the first <code>n_precalc</code> epochs. Defaults to None.</li>\n<li><strong>verbose:</strong>  Whether to show progress bars. Defaults to 0.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">smis</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">vocab</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">VocabProto</span>,</span><span class=\"param\">\t<span class=\"n\">rnd_func</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">RandomizeFunc</span>,</span><span class=\"param\">\t<span class=\"n\">n_jobs</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">init_epoch</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">just_shuffle</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_precalc</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span>)</span>"}, "lstm_clm.clm.randomized.RandomizedDataset.smis": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.smis", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.smis", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.RandomizedDataset.vocab": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.vocab", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.vocab", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.rnd_func", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.rnd_func", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.n_jobs", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.n_jobs", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.RandomizedDataset.epoch": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.epoch", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.epoch", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.just_shuffle", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.just_shuffle", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.n_precalc", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.n_precalc", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.RandomizedDataset.verbose": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.verbose", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.verbose", "kind": "variable", "doc": "<p></p>\n"}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.used", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.used", "kind": "variable", "doc": "<p>The number of epochs used.</p>\n", "annotation": ": int"}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.build_dataset", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.build_dataset", "kind": "function", "doc": "<p>Builds a tf.data.Dataset from the encoded SMILES.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batch_size:</strong>  The batch size.</li>\n<li><strong>strategy:</strong>  The strategy to use for distributed training. Defaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The built dataset.</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If the dataset is already used.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">strategy</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">distribute</span><span class=\"o\">.</span><span class=\"n\">distribute_lib</span><span class=\"o\">.</span><span class=\"n\">Strategy</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">randomized</span><span class=\"o\">.</span><span class=\"n\">BuiltRandomizedDataset</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.randomized_perplexity", "kind": "function", "doc": "<p>Calculate the perplexity of the randomized SMILES.</p>\n\n<ul>\n<li>Means over <code>N</code> randomizations per SMILES string.</li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gen:</strong>  The generator.</li>\n<li><strong>n_repr:</strong>  The number of times to randomize each SMILES string.</li>\n<li><strong>n_jobs:</strong>  The number of jobs to use for parallelization.</li>\n<li><strong>verbose:</strong>  Verbosity level. Defaults to 0.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The perplexity scores.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">multinomial</span><span class=\"o\">.</span><span class=\"n\">MultinomialCLM</span>,</span><span class=\"param\">\t<span class=\"n\">n_repr</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_jobs</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.on_epoch_begin", "kind": "function", "doc": "<p>Prepares the dataset for the next epoch.</p>\n\n<p>Called at the beginning of each epoch.\nRandomizes them if <code>just_shuffle</code> is False.\nEncodes them (with <code>just_shuffle</code> only in the first epoch).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.on_train_begin", "kind": "function", "doc": "<p>Prepares the dataset for training.</p>\n\n<p>Called at the beginning of training, if <code>n_precalc</code> is activated.\nShuffles the SMILES strings.\nRandomizes them (if not <code>just_shuffle</code>). Encodes them.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>epochs:</strong>  The final epoch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">epochs</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.encoded", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.encoded", "kind": "variable", "doc": "<p>The encoded data of the current epoch.</p>\n", "annotation": ": numpy.ndarray[typing.Any, numpy.dtype[numpy.int32]]"}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"fullname": "lstm_clm.clm.randomized.RandomizedDataset.build", "modulename": "lstm_clm.clm.randomized", "qualname": "RandomizedDataset.build", "kind": "function", "doc": "<p>Builds the train and val datasets for training.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>train:</strong>  The training data.</li>\n<li><strong>val:</strong>  The validation data.</li>\n<li><strong>vocab:</strong>  The vocabulary.</li>\n<li><strong>rnd_func:</strong>  The function to use for randomizing SMILES strings.</li>\n<li><strong>n_jobs:</strong>  The number of jobs.</li>\n<li><strong>init_epoch:</strong>  The initial epoch. Defaults to 0.</li>\n<li><strong>just_shuffle:</strong>  Whether to just shuffle the data. Defaults to False.</li>\n<li><strong>n_precalc:</strong>  If not None, precalcs the randomized SMILES\nfor the first <code>n_precalc</code> epochs. Defaults to None.</li>\n<li><strong>verbose:</strong>  Verbosity level. Defaults to 0.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The train and val datasets</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">train</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">vocab</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">VocabProto</span>,</span><span class=\"param\">\t<span class=\"n\">rnd_func</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">RandomizeFunc</span>,</span><span class=\"param\">\t<span class=\"n\">n_jobs</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">init_epoch</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">just_shuffle</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_precalc</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">typing_extensions</span><span class=\"o\">.</span><span class=\"n\">Self</span><span class=\"p\">,</span> <span class=\"n\">typing_extensions</span><span class=\"o\">.</span><span class=\"n\">Self</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils": {"fullname": "lstm_clm.clm.utils", "modulename": "lstm_clm.clm.utils", "kind": "module", "doc": "<p>Utility functions for the CLM generator.</p>\n"}, "lstm_clm.clm.utils.NullStrategy": {"fullname": "lstm_clm.clm.utils.NullStrategy", "modulename": "lstm_clm.clm.utils", "qualname": "NullStrategy", "kind": "class", "doc": "<p>Null strategy for single GPU training.</p>\n\n<p>As a placeholder for <code>tf.distribute.Strategy</code>.</p>\n"}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"fullname": "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset", "modulename": "lstm_clm.clm.utils", "qualname": "NullStrategy.experimental_distribute_dataset", "kind": "function", "doc": "<p>Returns the input dataset as is.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  The input dataset</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The input dataset</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"o\">~</span><span class=\"n\">DatasetT</span></span><span class=\"return-annotation\">) -> <span class=\"o\">~</span><span class=\"n\">DatasetT</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.NullStrategy.scope": {"fullname": "lstm_clm.clm.utils.NullStrategy.scope", "modulename": "lstm_clm.clm.utils", "qualname": "NullStrategy.scope", "kind": "function", "doc": "<p>Returns nullcontext.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A null context manager</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">contextlib</span><span class=\"o\">.</span><span class=\"n\">nullcontext</span><span class=\"p\">[</span><span class=\"kc\">None</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.allow_memory_growth": {"fullname": "lstm_clm.clm.utils.allow_memory_growth", "modulename": "lstm_clm.clm.utils", "qualname": "allow_memory_growth", "kind": "function", "doc": "<p>Allow memory growth for TensorFlow GPU devices.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.batch_tensor_slices": {"fullname": "lstm_clm.clm.utils.batch_tensor_slices", "modulename": "lstm_clm.clm.utils", "qualname": "batch_tensor_slices", "kind": "function", "doc": "<p>Batch tensor slices to <code>tf.data.Dataset</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>data:</strong>  Data to batch. Either a single tensor or a tuple! of tensors.</li>\n<li><strong>batch_size:</strong>  Batch size.</li>\n<li><strong>desc:</strong>  Description for tqdm. Defaults to None.</li>\n<li><strong>verbose:</strong>  Verbosity level. Defaults to 0.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A iterable over a <code>tf.data.Dataset</code> of batched data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"o\">~</span><span class=\"n\">BatchableT</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">desc</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Iterable</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">BatchableT</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.build_adam_optimizer": {"fullname": "lstm_clm.clm.utils.build_adam_optimizer", "modulename": "lstm_clm.clm.utils", "qualname": "build_adam_optimizer", "kind": "function", "doc": "<p>Return an Adam optimizer with the given learning rate.</p>\n\n<p>As there are some issues with the Adam optimizer on Apple M1,\nwe use the legacy version in this case.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>learning_rate:</strong>  Initial learning rate.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Adam optimizer.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">learning_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">optimizers</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.build_model_cp": {"fullname": "lstm_clm.clm.utils.build_model_cp", "modulename": "lstm_clm.clm.utils", "qualname": "build_model_cp", "kind": "function", "doc": "<p>Get the model checkpoint callback.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>models_path:</strong>  Path to save the models</li>\n<li><strong>scheme:</strong>  Scheme to save the models. Defaults to \"{epoch}.h5\".</li>\n<li><strong>best_only:</strong>  Save only the best model. Defaults to False.</li>\n<li><strong>weights_only:</strong>  Save only the weights. Defaults to True.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The model checkpoint callback</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">models_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">scheme</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;</span><span class=\"si\">{epoch}</span><span class=\"s1\">.h5&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">best_only</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">weights_only</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">keras</span><span class=\"o\">.</span><span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">ModelCheckpoint</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.categorical_sample": {"fullname": "lstm_clm.clm.utils.categorical_sample", "modulename": "lstm_clm.clm.utils", "qualname": "categorical_sample", "kind": "function", "doc": "<p>Sample from logits (tf.function wrapped).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>logits:</strong>  Logits. [Batch, Vocab] {float64}</li>\n<li><strong>temp:</strong>  Temperature factor. [1] {float64}</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Sampled tokens. [Batch] {int32}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">logits</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"o\">/</span>,</span><span class=\"param\">\t<span class=\"n\">temp</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.init_hidden_states": {"fullname": "lstm_clm.clm.utils.init_hidden_states", "modulename": "lstm_clm.clm.utils", "qualname": "init_hidden_states", "kind": "function", "doc": "<p>Initialize LSTM hidden states with zeros (tf.function wrapped).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>lstm_dims:</strong>  Neurons per LSTM layer.</li>\n<li><strong>batch_size:</strong>  Batch size.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Initialized hidden states. [[2 x [Batch, Hidden] {float32}], ...]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">lstm_dims</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.init_tensor_array": {"fullname": "lstm_clm.clm.utils.init_tensor_array", "modulename": "lstm_clm.clm.utils", "qualname": "init_tensor_array", "kind": "function", "doc": "<p>Initialize TensorArray for generated samples.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>dtype:</strong>  Data type (tf.int32 or tf.float32).</li>\n<li><strong>seq_len:</strong>  Sequence length.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Initialized TensorArray. [Length] {dtype}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">dtypes</span><span class=\"o\">.</span><span class=\"n\">DType</span>,</span><span class=\"param\">\t<span class=\"n\">seq_len</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">tensor_array_ops</span><span class=\"o\">.</span><span class=\"n\">TensorArray</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.lstm_model_call": {"fullname": "lstm_clm.clm.utils.lstm_model_call", "modulename": "lstm_clm.clm.utils", "qualname": "lstm_model_call", "kind": "function", "doc": "<p>Call a model with LSTM layers for one time step (tf.function wrapped).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layers:</strong>  Model layers.</li>\n<li><strong>lstm_dims:</strong>  Neurons per LSTM layer.</li>\n<li><strong>x_t:</strong>  Input at time step t.\nIf embedding: [Batch] {int32}\nElse: [Batch, Vocab] {float32}</li>\n<li><strong>hidden_states:</strong>  LSTM layers' hidden states. [2 x [Batch, Hidden] {float32}, ...]</li>\n<li><strong>training:</strong>  Whether in training mode. Defaults to False.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>tuple[tf.Tensor, list[list[tf.Tensor]], None]:\n      output: Time step t output. [Batch, Vocab] {float32}\n      states: LSTM layers' output states. [2 x [Batch, Hidden] {float32}, ...]\n      info: Always None.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">layers</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span>,</span><span class=\"param\">\t<span class=\"n\">lstm_dims</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">x_t</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_states</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">training</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]],</span> <span class=\"kc\">None</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.multinomial_sample": {"fullname": "lstm_clm.clm.utils.multinomial_sample", "modulename": "lstm_clm.clm.utils", "qualname": "multinomial_sample", "kind": "function", "doc": "<p>Sample from multinomial distribution (tf.function wrapped).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x_t:</strong>  Probability distribution at time step t. [Batch, Vocab] {float32}</li>\n<li><strong>temp:</strong>  Temperature factor. [1] {float64}</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Sampled tokens. [Batch] {int32}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x_t</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"o\">/</span>,</span><span class=\"param\">\t<span class=\"n\">temp</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span><span class=\"return-annotation\">) -> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, "lstm_clm.clm.utils.plot_history": {"fullname": "lstm_clm.clm.utils.plot_history", "modulename": "lstm_clm.clm.utils", "qualname": "plot_history", "kind": "function", "doc": "<p>Plot training history to file.</p>\n\n<p>Calculates the best epoch and loss from the validation data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>history_data:</strong>  Keras history object or dictionary of metrics</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Image object</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">history_data</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">clm</span><span class=\"o\">.</span><span class=\"n\">proto</span><span class=\"o\">.</span><span class=\"n\">HistoryProto</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">matplotlib</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"o\">.</span><span class=\"n\">Figure</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab": {"fullname": "lstm_clm.vocab", "modulename": "lstm_clm.vocab", "kind": "module", "doc": "<p>Vocabulary for the lstm_clm.</p>\n\n<h6 id=\"usage\">Usage:</h6>\n\n<blockquote>\n  <div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"c1\"># Import the vocabulary functions (extended triggers tensorflow import)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">lstm_clm.vocab</span> <span class=\"kn\">import</span> <span class=\"n\">Vocabulary</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"c1\"># Create a vocabulary instance</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">&quot;a&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;b&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;c&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;d&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;B&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;E&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;P&quot;</span><span class=\"p\">]</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">vocab</span> <span class=\"o\">=</span> <span class=\"n\">Vocabulary</span><span class=\"p\">(</span><span class=\"n\">tokens</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"s2\">&quot;P&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;B&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;E&quot;</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"c1\"># Encode to np.ndarray</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">encoded</span> <span class=\"o\">=</span> <span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">([</span><span class=\"s2\">&quot;abc&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;bcd&quot;</span><span class=\"p\">])</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">encoded</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># max len + 2 (for start and end token)</span>\n<span class=\"go\">(2, 7)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">encoded</span><span class=\"p\">)</span>\n<span class=\"go\">[[ 4 0 1 2 5 6 6</span>\n<span class=\"go\">   4 1 2 3 5 6 6]]</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"c1\"># Decode to list of strings</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">decoded</span> <span class=\"o\">=</span> <span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"n\">encoded</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">decoded</span><span class=\"p\">)</span>\n<span class=\"go\">[&quot;abc&quot;, &quot;bcd&quot;]</span>\n</code></pre>\n  </div>\n</blockquote>\n"}, "lstm_clm.vocab.VocabProto": {"fullname": "lstm_clm.vocab.VocabProto", "modulename": "lstm_clm.vocab", "qualname": "VocabProto", "kind": "class", "doc": "<p>Protocol for a vocabulary.</p>\n\n<p>Must implement the following attributes and methods:</p>\n\n<h6 id=\"attributes\">Attributes:</h6>\n\n<ul>\n<li><strong>tokens (list[str]):</strong>  List of unique tokens in the vocabulary.</li>\n<li><strong>max_len (int):</strong>  Maximum length of generated samples.</li>\n<li><strong>pad (str):</strong>  Padding token.</li>\n<li><strong>bos (str):</strong>  Beginning of sentence token.</li>\n<li><strong>eos (str | None):</strong>  End of sentence token. Optional.</li>\n<li><strong>unk (str | None):</strong>  Unknown token. Optional.</li>\n<li><strong>encode_map (dict[str, int]):</strong>  Token to index mapping.</li>\n<li><strong>decode_map (dict[int, str]):</strong>  Index to token mapping.</li>\n<li><strong>special_tokens (list[str]):</strong>  List of special tokens.</li>\n</ul>\n\n<h6 id=\"methods\">Methods:</h6>\n\n<blockquote>\n  <ul>\n  <li>__len__: Returns number of vocab tokens.</li>\n  <li>encode: Encode a list of tokens to indices.</li>\n  <li>decode: Decode a list of indices to tokens.</li>\n  </ul>\n</blockquote>\n", "bases": "typing.Protocol"}, "lstm_clm.vocab.VocabProto.__len__": {"fullname": "lstm_clm.vocab.VocabProto.__len__", "modulename": "lstm_clm.vocab", "qualname": "VocabProto.__len__", "kind": "function", "doc": "<p>@public Returns number of vocab tokens.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.VocabProto.encode": {"fullname": "lstm_clm.vocab.VocabProto.encode", "modulename": "lstm_clm.vocab", "qualname": "VocabProto.encode", "kind": "function", "doc": "<p>Encode a list of tokens to indices.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs:</strong>  Token inputs. [Batch] {str}</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Encoded indices. [Batch, Length] {int32}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">str_</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.VocabProto.decode": {"fullname": "lstm_clm.vocab.VocabProto.decode", "modulename": "lstm_clm.vocab", "qualname": "VocabProto.decode", "kind": "function", "doc": "<p>Decode a list of indices to tokens.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs:</strong>  Indices inputs. [Batch, Length] {int32}</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Decoded tokens. [Batch] {str}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.Vocabulary": {"fullname": "lstm_clm.vocab.Vocabulary", "modulename": "lstm_clm.vocab", "qualname": "Vocabulary", "kind": "class", "doc": "<p>Vocabulary class for tokenizing and encoding SMILES strings.</p>\n\n<p>Follows the <code>VocabProto</code> protocol.</p>\n\n<p>With different padding directions, other tokenization methods and more.\nWe have additional options in comparison to just the protocol.</p>\n\n<h6 id=\"attributes\">Attributes:</h6>\n\n<ul>\n<li><strong>tokens (list[str]):</strong>  Vocabulary to use for tokenization and encoding</li>\n<li><strong>max_len (int):</strong>  Maximum length of the model</li>\n<li><strong>pad (str):</strong>  Padding character</li>\n<li><strong>bos (str):</strong>  Start character</li>\n<li><strong>eos (str | None):</strong>  End character.</li>\n<li><strong>unk (str | None):</strong>  Unknown character. If None, <code>pad</code> is used.</li>\n<li><strong>encode_map (dict[str, int]):</strong>  Mapping from tokens to indices</li>\n<li><strong>decode_map (dict[int, str]):</strong>  Mapping from indices to tokens</li>\n<li><strong>special_tokens (list[str]):</strong>  List of special tokens</li>\n<li><strong>right_pad (bool):</strong>  If True, pads to the right, else pads to the left.</li>\n<li><strong>only_two_char_tokens (bool):</strong>  If True, only uses tokenization\nfor two character tokens ([Se] -> '[', 'Se' ']').</li>\n<li><strong>assert_length (bool):</strong>  If True, raises an error if\na SMILES string is longer than max_len_model.</li>\n<li><strong>assert_known (bool):</strong>  If True, raises an error if a token is unknown.</li>\n</ul>\n"}, "lstm_clm.vocab.Vocabulary.__init__": {"fullname": "lstm_clm.vocab.Vocabulary.__init__", "modulename": "lstm_clm.vocab", "qualname": "Vocabulary.__init__", "kind": "function", "doc": "<p>Initialize the vocabulary.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>tokens:</strong>  Vocabulary to use for tokenization and encoding</li>\n<li><strong>max_len:</strong>  Maximum length of the model</li>\n<li><strong>pad:</strong>  Padding character.</li>\n<li><strong>bos:</strong>  Start character.</li>\n<li><strong>eos:</strong>  End character. Defaults to None.</li>\n<li><strong>unk:</strong>  Unknown character. If None and assert_known is False, <code>pad</code> is used.\nDefaults to None.</li>\n<li><strong>right_pad:</strong>  If True, pads to the right, else pads to the left.\nDefaults to True.</li>\n<li><strong>only_two_char_tokens:</strong>  If True, only uses tokenization for two\ncharacter tokens ([Se] -> '[', 'Se' ']'). Defaults to False.</li>\n<li><strong>assert_length:</strong>  If True, raise if a SMILES string is longer than max_len.\nDefaults to False.</li>\n<li><strong>assert_known:</strong>  If True, raise if a token is unknown. Defaults to False.</li>\n</ul>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If tokens are not unique</li>\n<li><strong>ValueError:</strong>  If special tokens are not in the set of tokens</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokens</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">max_len</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">pad</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">bos</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">eos</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unk</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">right_pad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">only_two_char_tokens</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">assert_length</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">assert_known</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, "lstm_clm.vocab.Vocabulary.encode": {"fullname": "lstm_clm.vocab.Vocabulary.encode", "modulename": "lstm_clm.vocab", "qualname": "Vocabulary.encode", "kind": "function", "doc": "<p>Encode a list of tokens to indices.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs:</strong>  Token inputs. Can be a list/np.ndarray/tf.Tensor of strings.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Encoded indices. [Input Length x Sequence Length]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"s1\">&#39;EncodableBatch&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.Vocabulary.decode": {"fullname": "lstm_clm.vocab.Vocabulary.decode", "modulename": "lstm_clm.vocab", "qualname": "Vocabulary.decode", "kind": "function", "doc": "<p>Decode a list of indices to tokens.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs:</strong>  Indices inputs.\nCan be a list of integer lists or a np.ndarray/tf.Tensor (int32).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Decoded tokens. [Input Length]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">tensorflow</span><span class=\"o\">.</span><span class=\"n\">python</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.Vocabulary.__len__": {"fullname": "lstm_clm.vocab.Vocabulary.__len__", "modulename": "lstm_clm.vocab", "qualname": "Vocabulary.__len__", "kind": "function", "doc": "<p>@public Returns number of vocab tokens.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.decode": {"fullname": "lstm_clm.vocab.decode", "modulename": "lstm_clm.vocab.decode", "kind": "module", "doc": "<p>Decode functions for the vocabulary.</p>\n"}, "lstm_clm.vocab.decode.decode_single": {"fullname": "lstm_clm.vocab.decode.decode_single", "modulename": "lstm_clm.vocab.decode", "qualname": "decode_single", "kind": "function", "doc": "<p>Decode a single integer sequence to a string using a vocabulary.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>indices:</strong>  List of indices to decode. [Length] {int32}</li>\n<li><strong>decode_map:</strong>  Mapping from indices to tokens.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Joined string of decoded tokens. [1] {str}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">indices</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">decode_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.decode.build_decode_map": {"fullname": "lstm_clm.vocab.decode.build_decode_map", "modulename": "lstm_clm.vocab.decode", "qualname": "build_decode_map", "kind": "function", "doc": "<p>Adjusts the decode map if characters should be excluded.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>decode_map:</strong>  Mapping from indices to tokens.</li>\n<li><strong>encode_map:</strong>  Mapping from tokens to indices.</li>\n<li><strong>exclude:</strong>  List of values which are set to \"\" if found. Defaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Adjusted decode map.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">decode_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">encode_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">exclude</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.decode.decode_multi": {"fullname": "lstm_clm.vocab.decode.decode_multi", "modulename": "lstm_clm.vocab.decode", "qualname": "decode_multi", "kind": "function", "doc": "<p>Decode a nested integer sequence to a list of strings using a vocabulary.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs:</strong>  List of integers to decode. [Batch, Length] {int32}</li>\n<li><strong>decode_map:</strong>  Mapping from tokens to indices\nwith empty strings for excluded tokens.\nMay use <code>build_decode_map</code> to exclude special tokens.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Decoded list of integers as strings. [Batch] {str}</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If unknown character is in input.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">decode_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.decode.DecodeFunc": {"fullname": "lstm_clm.vocab.decode.DecodeFunc", "modulename": "lstm_clm.vocab.decode", "qualname": "DecodeFunc", "kind": "class", "doc": "<p>Decode function protocol.</p>\n", "bases": "typing.Protocol, typing.Generic[-T_contra]"}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"fullname": "lstm_clm.vocab.decode.DecodeFunc.__init__", "modulename": "lstm_clm.vocab.decode", "qualname": "DecodeFunc.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, "lstm_clm.vocab.decode.build_full_decode_func": {"fullname": "lstm_clm.vocab.decode.build_full_decode_func", "modulename": "lstm_clm.vocab.decode", "qualname": "build_full_decode_func", "kind": "function", "doc": "<p>Build a decode function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>adjusted_decode_map:</strong>  Decode map with empty strings for excluded tokens.</li>\n<li><strong>decode_func:</strong>  Decode function.\nTakes a decodable input and a decode map. Returns a list of strings.\nDefaults to <code>decode_multi</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">adjusted_decode_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">decode_func</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"o\">.</span><span class=\"n\">DecodeFunc</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"n\">T_contra</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"o\">-</span><span class=\"n\">T_contra</span><span class=\"p\">],</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]],</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.encode": {"fullname": "lstm_clm.vocab.encode", "modulename": "lstm_clm.vocab.encode", "kind": "module", "doc": "<p>Encode functions for the vocabulary.</p>\n"}, "lstm_clm.vocab.encode.WHITESPACE": {"fullname": "lstm_clm.vocab.encode.WHITESPACE", "modulename": "lstm_clm.vocab.encode", "qualname": "WHITESPACE", "kind": "variable", "doc": "<p>Regex pattern for whitespace</p>\n", "default_value": "re.compile(&#x27;\\\\s&#x27;)"}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"fullname": "lstm_clm.vocab.encode.REGEX_PATTERNS", "modulename": "lstm_clm.vocab.encode", "qualname": "REGEX_PATTERNS", "kind": "variable", "doc": "<p>Regex patterns for tokenization</p>\n\n<ul>\n<li>True: Keep Br, Cl, se, Se, Si, si, @@ together</li>\n<li>False: Keep Br, Cl, @@, %{number} and everything in [] together</li>\n<li>None: Split character-wise</li>\n</ul>\n", "annotation": ": dict[bool | None, lstm_clm.vocab.encode.SplitProto]", "default_value": "{True: re.compile(&#x27;(Br|Cl|se|Se|Si|si|@@)&#x27;), False: re.compile(&#x27;(\\\\[[^]]*\\\\]|%\\\\\\\\d{2}|Br|Cl|@@)&#x27;), None: re.compile(&#x27;\\\\s&#x27;)}"}, "lstm_clm.vocab.encode.SplitProto": {"fullname": "lstm_clm.vocab.encode.SplitProto", "modulename": "lstm_clm.vocab.encode", "qualname": "SplitProto", "kind": "class", "doc": "<p>Protocol for splitting a string for tokenization.</p>\n\n<p>Main implementation is <code>re.Pattern</code>.\nCustom implementation needs a <code>split</code> method\nwhich seperates tokens that should\nbe kept together from parts that\nshould be split character-wise.\nThe tokens should be in every second\nposition.</p>\n\n<h6 id=\"example\">Example:</h6>\n\n<blockquote>\n  <div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">splitter</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"s2\">&quot;(Br|Cl|se|Se|Si|si|@@)&quot;</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">splitter</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s2\">&quot;BrCCCCl&quot;</span><span class=\"p\">)</span>\n<span class=\"go\">[&quot;Br&quot;, &quot;CCC&quot;, &quot;Cl&quot;]</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">splitter</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s2\">&quot;CCCClCCC&quot;</span><span class=\"p\">)</span>\n<span class=\"go\">[&quot;&quot;, &quot;CCC, &quot;Cl&quot;, &quot;CCC&quot;]</span>\n</code></pre>\n  </div>\n</blockquote>\n", "bases": "typing.Protocol"}, "lstm_clm.vocab.encode.SplitProto.__init__": {"fullname": "lstm_clm.vocab.encode.SplitProto.__init__", "modulename": "lstm_clm.vocab.encode", "qualname": "SplitProto.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, "lstm_clm.vocab.encode.SplitProto.split": {"fullname": "lstm_clm.vocab.encode.SplitProto.split", "modulename": "lstm_clm.vocab.encode", "qualname": "SplitProto.split", "kind": "function", "doc": "<p>Split a string into tokens and characters.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  String to split</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Iterable of tokens and characters</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Iterable</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.encode.pad_tokens": {"fullname": "lstm_clm.vocab.encode.pad_tokens", "modulename": "lstm_clm.vocab.encode", "qualname": "pad_tokens", "kind": "function", "doc": "<p>Pads a tokenized string to a given length.</p>\n\n<p>Pads either before or after the input tokens.\nCuts off the input tokens if they are longer than max_len.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>tokens:</strong>  List of tokens. [Any] {str}</li>\n<li><strong>pad:</strong>  Padding character.</li>\n<li><strong>max_len_model:</strong>  Maximum length of the model.\nCaveat: add 1 for each of start and/or end token.</li>\n<li><strong>right_pad:</strong>  If True, pads to the right, else pads to the left.\nDefaults to True.</li>\n<li><strong>assert_length:</strong>  If true, raises error on too long input.\nElse, cuts off the input. Defaults to False.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Padded list of tokenized strings [Batch, Length] {str}</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If <code>assert_length</code> is true and number\nof tokens is larger than <code>max_len_model</code></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokens</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">pad</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">max_len_model</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">right_pad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">assert_length</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.encode.tokenize_string": {"fullname": "lstm_clm.vocab.encode.tokenize_string", "modulename": "lstm_clm.vocab.encode", "qualname": "tokenize_string", "kind": "function", "doc": "<p>Split a string into tokens. Encloses them with BOS and EOS tokens, if present.</p>\n\n<h6 id=\"example\">Example:</h6>\n\n<blockquote>\n  <div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pattern</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s2\">&quot;(\\d+)&quot;</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">bos</span><span class=\"p\">,</span> <span class=\"n\">eos</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;BOS&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;EOS&quot;</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">tokenize_string</span><span class=\"p\">(</span><span class=\"s2\">&quot;t33st&quot;</span><span class=\"p\">,</span> <span class=\"n\">pattern</span><span class=\"p\">,</span> <span class=\"n\">bos</span><span class=\"p\">,</span> <span class=\"n\">eos</span><span class=\"p\">)</span>\n<span class=\"gp\">... </span><span class=\"p\">[</span><span class=\"s2\">&quot;BOS&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;t&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;33&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;s&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;t&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;EOS&quot;</span><span class=\"p\">]</span>\n</code></pre>\n  </div>\n</blockquote>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs:</strong>  String to split. [1] {str}</li>\n<li><strong>pattern:</strong>  Pattern to split by (e.g. regex pattern)\nEvery second element is a token.\nEvery other element is character-wise split.</li>\n<li><strong>bos:</strong>  Start token</li>\n<li><strong>eos:</strong>  End token. Optional.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>List of tokens. [Any] {str}</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If input contains whitespace</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">pattern</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"o\">.</span><span class=\"n\">SplitProto</span>,</span><span class=\"param\">\t<span class=\"n\">bos</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">eos</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.encode.encode_single": {"fullname": "lstm_clm.vocab.encode.encode_single", "modulename": "lstm_clm.vocab.encode", "qualname": "encode_single", "kind": "function", "doc": "<p>Encode a single padded token sequence to integers using a vocabulary.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>tokens:</strong>  List of padded tokens to encode. [Length] {str}</li>\n<li><strong>encode_map:</strong>  Mapping from tokens to indices.</li>\n<li><strong>unk:</strong>  Unknown character. If None, raises error on unknown characters.\nDefaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Encoded list of tokens as list of ints. [Length] {int32}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokens</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">encode_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">unk</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.encode.encode_multi": {"fullname": "lstm_clm.vocab.encode.encode_multi", "modulename": "lstm_clm.vocab.encode", "qualname": "encode_multi", "kind": "function", "doc": "<p>Encode a list of padded tokens to integers using a vocabulary.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs:</strong>  Tokenized and padded inputs for encoding. [Batch, Length] {str}</li>\n<li><strong>encode_map:</strong>  Mapping from tokens to indices</li>\n<li><strong>unk:</strong>  Unknown character. Defaults to None.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Encoded list of strings as int array [Batch, Length] {int32}</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">str_</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">encode_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">unk</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.encode.build_tokenize_func": {"fullname": "lstm_clm.vocab.encode.build_tokenize_func", "modulename": "lstm_clm.vocab.encode", "qualname": "build_tokenize_func", "kind": "function", "doc": "<p>Build a tokenization function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>bos:</strong>  Beginning of sentence token.</li>\n<li><strong>eos:</strong>  End of sentence token. If None, no end token is added.</li>\n<li><strong>pattern:</strong>  Pattern to split by (e.g. regex pattern)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Tokenization function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">bos</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">eos</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pattern</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"o\">.</span><span class=\"n\">SplitProto</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.encode.build_pad_func": {"fullname": "lstm_clm.vocab.encode.build_pad_func", "modulename": "lstm_clm.vocab.encode", "qualname": "build_pad_func", "kind": "function", "doc": "<p>Build a padding function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>max_len_model:</strong>  Maximum length of the model</li>\n<li><strong>pad:</strong>  Padding character</li>\n<li><strong>right_pad:</strong>  If True, pads to the right, else pads to the left.</li>\n<li><strong>assert_length:</strong>  If true, raises error instead of cutting. Defaults to False.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Padding function.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">max_len_model</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">pad</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">right_pad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">assert_length</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.encode.build_encode_func": {"fullname": "lstm_clm.vocab.encode.build_encode_func", "modulename": "lstm_clm.vocab.encode", "qualname": "build_encode_func", "kind": "function", "doc": "<p>Build an encode function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>encode_map:</strong>  Token to index mapping.</li>\n<li><strong>unk:</strong>  Unknown token.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">encode_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">unk</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">str_</span><span class=\"p\">]]]],</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.encode.encode_data": {"fullname": "lstm_clm.vocab.encode.encode_data", "modulename": "lstm_clm.vocab.encode", "qualname": "encode_data", "kind": "function", "doc": "<p>Encodes strings to integers using a defined tokenization, padding and encoding.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inputs:</strong>  List of strings. [Batch] {str}</li>\n<li><strong>tokenize_func:</strong>  Tokenization function.\nTakes a single string and returns a list of tokens.\nAdjustments, like BOS and EOS tokens, should be done here.\nMay use <code>build_tokenize_func</code> to build.</li>\n<li><strong>pad_func:</strong>  Padding function.\nTakes a list of tokens and returns a padded list of tokens.\nPad direction and length must be defined in the function.\nMay use <code>build_pad_func</code> to build.</li>\n<li><strong>encode_func:</strong>  Encoding function.\nTakes a list of padded tokens and returns an encoded list of tokens.\nMapping from\nMay use <code>build_encode_func</code> to build.</li>\n<li><strong>max_len_model:</strong>  Maximum length of the model.\nHas to include BOS and EOS tokens if present.\nIf set and any input is longer, an error is raised.\nElse, the input is cut off.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Encoded SMILES strings</p>\n</blockquote>\n\n<h6 id=\"raises\">Raises:</h6>\n\n<ul>\n<li><strong>ValueError:</strong>  If <code>max_len_model</code> is set\nand any input is longer than <code>max_len_model</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">str_</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">tokenize_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">pad_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">encode_func</span><span class=\"p\">:</span> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">str_</span><span class=\"p\">]]]],</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]]</span>,</span><span class=\"param\">\t<span class=\"n\">max_len_model</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, "lstm_clm.vocab.encode.build_full_encode_func": {"fullname": "lstm_clm.vocab.encode.build_full_encode_func", "modulename": "lstm_clm.vocab.encode", "qualname": "build_full_encode_func", "kind": "function", "doc": "<p>Build an encode function.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>encode_map:</strong>  Token to index mapping.</li>\n<li><strong>pad:</strong>  Padding character.</li>\n<li><strong>bos:</strong>  Beginning of sentence token.</li>\n<li><strong>eos:</strong>  End of sentence token. If None, no end token is added.</li>\n<li><strong>unk:</strong>  Unknown token.</li>\n<li><strong>max_len:</strong>  Maximum length of the model.</li>\n<li><strong>pattern:</strong>  Pattern to split by (e.g. regex pattern)</li>\n<li><strong>right_pad:</strong>  If True, pads to the right, else pads to the left.</li>\n<li><strong>assert_length:</strong>  If true, raises error instead of cutting. Defaults to False.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">encode_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">pad</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">bos</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">eos</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">unk</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pattern</span><span class=\"p\">:</span> <span class=\"n\">lstm_clm</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"o\">.</span><span class=\"n\">SplitProto</span>,</span><span class=\"param\">\t<span class=\"n\">max_len</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">right_pad</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">assert_length</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span><span class=\"p\">[[</span><span class=\"n\">Sequence</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">dtype</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">]]]</span>:</span></span>", "funcdef": "def"}}, "docInfo": {"lstm_clm": {"qualname": 0, "fullname": 2, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 27}, "lstm_clm.callbacks": {"qualname": 0, "fullname": 3, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 8}, "lstm_clm.callbacks.JSDCallback": {"qualname": 1, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 4, "doc": 147}, "lstm_clm.callbacks.JSDCallback.__init__": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 169, "bases": 0, "doc": 82}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 291, "bases": 0, "doc": 96}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"qualname": 4, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 47, "bases": 0, "doc": 50}, "lstm_clm.callbacks.jensen_shannon_divergence": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 27, "bases": 0, "doc": 111}, "lstm_clm.clm": {"qualname": 0, "fullname": 3, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 12}, "lstm_clm.clm.BaseCLM": {"qualname": 1, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 167}, "lstm_clm.clm.BaseCLM.__init__": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 126, "bases": 0, "doc": 155}, "lstm_clm.clm.BaseCLM.from_vocab": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 79, "bases": 0, "doc": 50}, "lstm_clm.clm.BaseCLM.call_cell": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 179, "bases": 0, "doc": 102}, "lstm_clm.clm.BaseCLM.load_optim": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 99, "bases": 0, "doc": 76}, "lstm_clm.clm.BaseCLM.save_optim": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 41, "bases": 0, "doc": 47}, "lstm_clm.clm.MultinomialCLM": {"qualname": 1, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 5, "doc": 101}, "lstm_clm.clm.MultinomialCLM.generate": {"qualname": 2, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 224, "bases": 0, "doc": 140}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"qualname": 4, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 115, "bases": 0, "doc": 68}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"qualname": 4, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 214, "bases": 0, "doc": 148}, "lstm_clm.clm.Trainer": {"qualname": 1, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 14}, "lstm_clm.clm.Trainer.fit": {"qualname": 2, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 209, "bases": 0, "doc": 88}, "lstm_clm.clm.Trainer.callbacks": {"qualname": 2, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 340, "bases": 0, "doc": 198}, "lstm_clm.clm.Trainer.init": {"qualname": 2, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 184, "bases": 0, "doc": 72}, "lstm_clm.clm.Trainer.build": {"qualname": 2, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 239, "bases": 0, "doc": 156}, "lstm_clm.clm.VocabMultinomialCLM": {"qualname": 1, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 5, "doc": 97}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 1, "signature": 0, "bases": 0, "doc": 10}, "lstm_clm.clm.allow_memory_growth": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 10, "bases": 0, "doc": 10}, "lstm_clm.clm.batch_tensor_slices": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 84, "bases": 0, "doc": 91}, "lstm_clm.clm.build_adam_optimizer": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 40, "bases": 0, "doc": 64}, "lstm_clm.clm.build_model_cp": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 118, "bases": 0, "doc": 83}, "lstm_clm.clm.get_data_from_vocab": {"qualname": 4, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 59, "bases": 0, "doc": 66}, "lstm_clm.clm.perplexity": {"qualname": 0, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 9}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"qualname": 4, "fullname": 8, "annotation": 0, "default_value": 0, "signature": 65, "bases": 0, "doc": 31}, "lstm_clm.clm.perplexity.build_hash_table": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 54, "bases": 0, "doc": 46}, "lstm_clm.clm.perplexity.calc_perplexity": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 122, "bases": 0, "doc": 67}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 250, "bases": 0, "doc": 121}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 307, "bases": 0, "doc": 136}, "lstm_clm.clm.randomized": {"qualname": 0, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 8}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"qualname": 1, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 109}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 114, "bases": 0, "doc": 66}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.steps": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.RandomizedDataset": {"qualname": 1, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 411}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 169, "bases": 0, "doc": 137}, "lstm_clm.clm.randomized.RandomizedDataset.smis": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.RandomizedDataset.vocab": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.RandomizedDataset.epoch": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.RandomizedDataset.verbose": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 3}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"qualname": 2, "fullname": 6, "annotation": 2, "default_value": 0, "signature": 0, "bases": 0, "doc": 8}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 88, "bases": 0, "doc": 77}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 104, "bases": 0, "doc": 96}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"qualname": 4, "fullname": 8, "annotation": 0, "default_value": 0, "signature": 14, "bases": 0, "doc": 41}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"qualname": 4, "fullname": 8, "annotation": 0, "default_value": 0, "signature": 24, "bases": 0, "doc": 54}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"qualname": 2, "fullname": 6, "annotation": 7, "default_value": 0, "signature": 0, "bases": 0, "doc": 10}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 221, "bases": 0, "doc": 149}, "lstm_clm.clm.utils": {"qualname": 0, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 9}, "lstm_clm.clm.utils.NullStrategy": {"qualname": 1, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 21}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"qualname": 4, "fullname": 8, "annotation": 0, "default_value": 0, "signature": 28, "bases": 0, "doc": 36}, "lstm_clm.clm.utils.NullStrategy.scope": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 25, "bases": 0, "doc": 18}, "lstm_clm.clm.utils.allow_memory_growth": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 10, "bases": 0, "doc": 10}, "lstm_clm.clm.utils.batch_tensor_slices": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 84, "bases": 0, "doc": 91}, "lstm_clm.clm.utils.build_adam_optimizer": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 40, "bases": 0, "doc": 64}, "lstm_clm.clm.utils.build_model_cp": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 118, "bases": 0, "doc": 83}, "lstm_clm.clm.utils.categorical_sample": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 77, "bases": 0, "doc": 47}, "lstm_clm.clm.utils.init_hidden_states": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 69, "bases": 0, "doc": 57}, "lstm_clm.clm.utils.init_tensor_array": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 74, "bases": 0, "doc": 50}, "lstm_clm.clm.utils.lstm_model_call": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 196, "bases": 0, "doc": 127}, "lstm_clm.clm.utils.multinomial_sample": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 78, "bases": 0, "doc": 54}, "lstm_clm.clm.utils.plot_history": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 69, "bases": 0, "doc": 52}, "lstm_clm.vocab": {"qualname": 0, "fullname": 3, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 430}, "lstm_clm.vocab.VocabProto": {"qualname": 1, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 2, "doc": 187}, "lstm_clm.vocab.VocabProto.__len__": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 14, "bases": 0, "doc": 9}, "lstm_clm.vocab.VocabProto.encode": {"qualname": 2, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 111, "bases": 0, "doc": 40}, "lstm_clm.vocab.VocabProto.decode": {"qualname": 2, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 110, "bases": 0, "doc": 40}, "lstm_clm.vocab.Vocabulary": {"qualname": 1, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 253}, "lstm_clm.vocab.Vocabulary.__init__": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 181, "bases": 0, "doc": 205}, "lstm_clm.vocab.Vocabulary.encode": {"qualname": 2, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 66, "bases": 0, "doc": 49}, "lstm_clm.vocab.Vocabulary.decode": {"qualname": 2, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 110, "bases": 0, "doc": 51}, "lstm_clm.vocab.Vocabulary.__len__": {"qualname": 3, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 14, "bases": 0, "doc": 9}, "lstm_clm.vocab.decode": {"qualname": 0, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 8}, "lstm_clm.vocab.decode.decode_single": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 48, "bases": 0, "doc": 61}, "lstm_clm.vocab.decode.build_decode_map": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 99, "bases": 0, "doc": 74}, "lstm_clm.vocab.decode.decode_multi": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 102, "bases": 0, "doc": 101}, "lstm_clm.vocab.decode.DecodeFunc": {"qualname": 1, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 0, "bases": 6, "doc": 6}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 18, "bases": 0, "doc": 3}, "lstm_clm.vocab.decode.build_full_decode_func": {"qualname": 4, "fullname": 8, "annotation": 0, "default_value": 0, "signature": 175, "bases": 0, "doc": 58}, "lstm_clm.vocab.encode": {"qualname": 0, "fullname": 4, "annotation": 0, "default_value": 0, "signature": 0, "bases": 0, "doc": 8}, "lstm_clm.vocab.encode.WHITESPACE": {"qualname": 1, "fullname": 5, "annotation": 0, "default_value": 6, "signature": 0, "bases": 0, "doc": 6}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"qualname": 2, "fullname": 6, "annotation": 9, "default_value": 19, "signature": 0, "bases": 0, "doc": 41}, "lstm_clm.vocab.encode.SplitProto": {"qualname": 1, "fullname": 5, "annotation": 0, "default_value": 0, "signature": 0, "bases": 2, "doc": 182}, "lstm_clm.vocab.encode.SplitProto.__init__": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 18, "bases": 0, "doc": 3}, "lstm_clm.vocab.encode.SplitProto.split": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 30, "bases": 0, "doc": 39}, "lstm_clm.vocab.encode.pad_tokens": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 94, "bases": 0, "doc": 174}, "lstm_clm.vocab.encode.tokenize_string": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 81, "bases": 0, "doc": 300}, "lstm_clm.vocab.encode.encode_single": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 80, "bases": 0, "doc": 83}, "lstm_clm.vocab.encode.encode_multi": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 157, "bases": 0, "doc": 76}, "lstm_clm.vocab.encode.build_tokenize_func": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 80, "bases": 0, "doc": 67}, "lstm_clm.vocab.encode.build_pad_func": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 91, "bases": 0, "doc": 80}, "lstm_clm.vocab.encode.build_encode_func": {"qualname": 3, "fullname": 7, "annotation": 0, "default_value": 0, "signature": 147, "bases": 0, "doc": 33}, "lstm_clm.vocab.encode.encode_data": {"qualname": 2, "fullname": 6, "annotation": 0, "default_value": 0, "signature": 300, "bases": 0, "doc": 225}, "lstm_clm.vocab.encode.build_full_encode_func": {"qualname": 4, "fullname": 8, "annotation": 0, "default_value": 0, "signature": 216, "bases": 0, "doc": 129}}, "length": 106, "save": true}, "index": {"qualname": {"root": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}}, "df": 9, "j": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}}, "df": 1, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "k": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}}, "df": 4}}}}}}}}}}, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}, "o": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"tf": 1}}, "df": 1}}}, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"tf": 1}}, "df": 1}}}}, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}}, "df": 11}}}}, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}}, "df": 2}, "n": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 3}}}}}}}, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 2}}}}}}, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 3}, "p": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 2, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}}}}}}}, "e": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.epoch": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}}, "df": 4}}}}, "n": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}}, "df": 1}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 7, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}}, "df": 1}}}}}}, "x": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}}, "df": 1}}}}}}}}}}}}, "s": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}, "u": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"tf": 1}}, "df": 1}}}}}}, "a": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 1}}, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 3, "s": {"docs": {"lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 1}}}}}}, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 2}}}}}, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"tf": 1}}, "df": 1}}, "n": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}}, "df": 2}}}}}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.steps": {"tf": 1}}, "df": 1}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.utils.init_hidden_states": {"tf": 1}}, "df": 1}}}}, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 1}}}}}, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.smis": {"tf": 1}}, "df": 1}}}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}}, "df": 1}}}}, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}}, "df": 1, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}}, "df": 3}}}}}}}}}}, "d": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}}, "df": 1}}}}}}}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {"lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 2, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}}, "df": 3}}}}}}, "e": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}}, "df": 1}}}}}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}}, "df": 6, "f": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}}, "df": 2}}}}}}}}}}, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 6}}}}}, "t": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 3}}}}, "u": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 15}, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.steps": {"tf": 1}}, "df": 6}}}}}}}}}}}}}}}}}}}}}, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 2}}}}}, "f": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 3}}}, "i": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1}}, "df": 1}}, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 6}}, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 2}}}}, "v": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "b": {"docs": {"lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.vocab": {"tf": 1}}, "df": 4, "m": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}}, "df": 2}}}}}}}}}}}}}}, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}}, "df": 4}}}}}, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}}, "df": 5}}}}}}}}}, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.verbose": {"tf": 1}}, "df": 1}}}}}}}, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 2, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}}}}}, "c": {"docs": {"lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}}, "df": 1}}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.utils.categorical_sample": {"tf": 1}}, "df": 1}}}}}}}}}}, "e": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}}, "df": 1}}}, "p": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 2}}, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}}, "df": 1}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 1}}}, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}}, "df": 2}}}, "m": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {"lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}}, "df": 2, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 1, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}}, "df": 4}}}}}}}}}}}}}, "e": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 2}}}}}, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 3}}}}, "a": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.vocab.decode.build_decode_map": {"tf": 1}}, "df": 1}}}, "n": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"tf": 1}}, "df": 2, "e": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}}, "df": 1}}}, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}}, "df": 3}}}}}}}}}}}}, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 2}, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}}, "df": 2}}}}}}}, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 1, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.Trainer": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}}, "df": 5}}}}}}, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 3}}}}}, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}}, "df": 1}}}}}, "w": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 2}}}, "h": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}}, "df": 1}}}}}}}}}}, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 5}}}}}}}}}, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}}, "df": 1}}}, "o": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}}, "df": 1}}}, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"tf": 1}}, "df": 1}}}}}}, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 1}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}}, "d": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}}, "df": 2}}}, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}}, "df": 1}}}}}, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 2}}}}, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 1}}}}}, "h": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}}, "df": 1}}}, "i": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.utils.init_hidden_states": {"tf": 1}}, "df": 1}}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 1}}}}}}}, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 1, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.smis": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.vocab": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.epoch": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.verbose": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 17}}}}}}}}}}}}}}}}, "n": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"tf": 1}}, "df": 1}}, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}}, "df": 1}}}}}}, "fullname": {"root": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}}, "df": 9, "l": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm": {"tf": 1}, "lstm_clm.callbacks": {"tf": 1}, "lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.steps": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.smis": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.vocab": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.epoch": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.verbose": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode": {"tf": 1}, "lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 106}}}, "o": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}}, "df": 1}}}, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}}, "df": 2}}}, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm": {"tf": 1}, "lstm_clm.callbacks": {"tf": 1}, "lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.init": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1.4142135623730951}, "lstm_clm.clm.allow_memory_growth": {"tf": 1.4142135623730951}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.steps": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.smis": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.vocab": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.epoch": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.verbose": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.plot_history": {"tf": 1.4142135623730951}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode": {"tf": 1}, "lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 106}}, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 2, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks": {"tf": 1}, "lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 7}}}}}}, "c": {"docs": {"lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}}, "df": 1}}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.utils.categorical_sample": {"tf": 1}}, "df": 1}}}}}}}}}}, "e": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}}, "df": 1}}}, "p": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 2}}, "j": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}}, "df": 1, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "k": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}}, "df": 4}}}}}}}}}}, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}, "o": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"tf": 1}}, "df": 1}}}, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"tf": 1}}, "df": 1}}}}, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}}, "df": 11}}}}, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}}, "df": 2}, "n": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 3}}}}}}}, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 2}}}}}}, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 3}, "p": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 2, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}}}}}}}, "e": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.epoch": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}}, "df": 4}}}}, "n": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}}, "df": 1}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.encode": {"tf": 1}, "lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 17, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}}, "df": 1}}}}}}, "x": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}}, "df": 1}}}}}}}}}}}}, "s": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}, "u": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"tf": 1}}, "df": 1}}}}}}, "a": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 1}}, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 3, "s": {"docs": {"lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 1}}}}}}, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 2}}}}}, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"tf": 1}}, "df": 1}}, "n": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}}, "df": 2}}}}}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.steps": {"tf": 1}}, "df": 1}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.utils.init_hidden_states": {"tf": 1}}, "df": 1}}}}, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 1}}}}}, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.smis": {"tf": 1}}, "df": 1}}}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}}, "df": 1}}}}, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}}, "df": 1, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}}, "df": 3}}}}}}}}}}, "d": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}}, "df": 1}}}}}}}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {"lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 2, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}}, "df": 3}}}}}}, "e": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}}, "df": 1}}}}}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.4142135623730951}}, "df": 9, "f": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}}, "df": 2}}}}}}}}}}, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 6}}}}}, "t": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 3}}}}, "u": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 15}, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.steps": {"tf": 1}}, "df": 6}}}}}}}}}}}}}}}}}}}}}, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 2}}}}}, "f": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 3}}}, "i": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1}}, "df": 1}}, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 6}}, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 2}}}}, "v": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "b": {"docs": {"lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.vocab": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode": {"tf": 1}, "lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 36, "m": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}}, "df": 2}}}}}}}}}}}}}}, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}}, "df": 4}}}}}, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}}, "df": 5}}}}}}}}}, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.verbose": {"tf": 1}}, "df": 1}}}}}}}, "m": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {"lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}}, "df": 2, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 1, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}}, "df": 4}}}}}}}}}}}}}, "e": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 2}}}}}, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 3}}}}, "a": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.vocab.decode.build_decode_map": {"tf": 1}}, "df": 1}}}, "n": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"tf": 1}}, "df": 2, "e": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}}, "df": 1}}}, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}}, "df": 3}}}}}}}}}}}}, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 2}, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}}, "df": 2}}}}}}}, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 1, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.Trainer": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}}, "df": 5}}}}}}, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 3}}}}}, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}}, "df": 1}}}}}, "w": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 2}}}, "h": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}}, "df": 1}}}}}}}}}}, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 8}}}}}}}}}, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}}, "df": 1}}}, "o": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}}, "df": 1}}}, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"tf": 1}}, "df": 1}}}}}}, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 1}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}}, "d": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}}, "df": 2}}}, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}}, "df": 1}}}}}, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 2}}}}, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 1}}}}}, "h": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}}, "df": 1}}}, "i": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.utils.init_hidden_states": {"tf": 1}}, "df": 1}}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 1}}}}}}}, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.randomized": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.steps": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.smis": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.vocab": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.epoch": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.verbose": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 24, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.smis": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.vocab": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.epoch": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.verbose": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 17}}}}}}}}}}}}}}}}, "n": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"tf": 1}}, "df": 1}}, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}}, "df": 1}}}, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.utils": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 14}}}}}}}, "annotation": {"root": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1.4142135623730951}}, "df": 3, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"3": {"2": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "docs": {"lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}}, "df": 1}}}, "n": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1.4142135623730951}}, "df": 1}}}}, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {}, "df": 0, "[": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "y": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}}, "df": 1}}}}}}}}}}}}}, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}}, "df": 1}}}, "d": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "y": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "[": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}}, "df": 1}}}}}}}}}}, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "[": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}}}}}, "l": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}, "v": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "b": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}}, "s": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}}}}}}}}, "default_value": {"root": {"docs": {"lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 2, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 2}}}}, "r": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1.7320508075688772}}, "df": 2}}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1.7320508075688772}}, "df": 2}}}}}}}, "x": {"2": {"7": {"docs": {"lstm_clm.vocab.encode.WHITESPACE": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 2.449489742783178}}, "df": 2}, "docs": {}, "df": 0}, "docs": {}, "df": 0}, "s": {"docs": {"lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 2}, "b": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "i": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}}}}}}}}}}}}}, "f": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}, "d": {"docs": {}, "df": 0, "{": {"2": {"docs": {}, "df": 0, "}": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}}}}, "docs": {}, "df": 0}}, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}}, "signature": {"root": {"0": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 12}, "1": {"0": {"0": {"0": {"0": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "docs": {}, "df": 0}, "2": {"4": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 4}, "docs": {}, "df": 0}, "docs": {}, "df": 0}, "2": {"8": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 4}, "3": {"9": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1.4142135623730951}}, "df": 3}, "docs": {}, "df": 0}, "docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 11.661903789690601}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 15.362291495737216}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 6.244997998398398}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 4.795831523312719}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 10}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 7.937253933193772}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 12.041594578792296}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 9.1104335791443}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 5.830951894845301}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 13.379088160259652}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 9.695359714832659}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 13.114877048604}, "lstm_clm.clm.Trainer.fit": {"tf": 12.922847983320086}, "lstm_clm.clm.Trainer.callbacks": {"tf": 16.522711641858304}, "lstm_clm.clm.Trainer.init": {"tf": 12.041594578792296}, "lstm_clm.clm.Trainer.build": {"tf": 13.784048752090222}, "lstm_clm.clm.allow_memory_growth": {"tf": 3}, "lstm_clm.clm.batch_tensor_slices": {"tf": 8.366600265340756}, "lstm_clm.clm.build_adam_optimizer": {"tf": 5.656854249492381}, "lstm_clm.clm.build_model_cp": {"tf": 9.695359714832659}, "lstm_clm.clm.get_data_from_vocab": {"tf": 6.928203230275509}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 7.211102550927978}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 6.557438524302}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 9.9498743710662}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 14.106735979665885}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 15.716233645501712}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 9.539392014169456}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 11.575836902790225}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 8.366600265340756}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 9.1104335791443}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 3.4641016151377544}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 4.47213595499958}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 13.228756555322953}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 4.898979485566356}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 4.58257569495584}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 3}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 8.366600265340756}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 5.656854249492381}, "lstm_clm.clm.utils.build_model_cp": {"tf": 9.695359714832659}, "lstm_clm.clm.utils.categorical_sample": {"tf": 8}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 7.416198487095663}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 7.615773105863909}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 12.529964086141668}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 8}, "lstm_clm.clm.utils.plot_history": {"tf": 7.416198487095663}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 3.4641016151377544}, "lstm_clm.vocab.VocabProto.encode": {"tf": 9.539392014169456}, "lstm_clm.vocab.VocabProto.decode": {"tf": 9.486832980505138}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 12.041594578792296}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 7.280109889280518}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 9.486832980505138}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 3.4641016151377544}, "lstm_clm.vocab.decode.decode_single": {"tf": 6.244997998398398}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 9}, "lstm_clm.vocab.decode.decode_multi": {"tf": 9.1104335791443}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 4}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 11.789826122551595}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 4}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 5}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 8.660254037844387}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 8.12403840463596}, "lstm_clm.vocab.encode.encode_single": {"tf": 8.12403840463596}, "lstm_clm.vocab.encode.encode_multi": {"tf": 11.313708498984761}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 8.06225774829855}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 8.48528137423857}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 10.908712114635714}, "lstm_clm.vocab.encode.encode_data": {"tf": 15.491933384829668}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 13.152946437965905}}, "df": 68, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 2}}, "p": {"docs": {}, "df": 0, "u": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}}, "df": 1}}}, "l": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.init": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.build": {"tf": 2}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 23}}}, "o": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}}, "df": 1}, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.utils.categorical_sample": {"tf": 1}}, "df": 1}}}}, "s": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.4142135623730951}}, "df": 1, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}}, "df": 1}}}}, "o": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 5}}}}}, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 7, "g": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 4}}}}, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}}}}}, "i": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}}, "df": 15}}, "b": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}}, "df": 3}}, "a": {"docs": {}, "df": 0, "y": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 1}}}}}}, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 2}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 2}, "lstm_clm.clm.Trainer.fit": {"tf": 2}, "lstm_clm.clm.Trainer.callbacks": {"tf": 2.6457513110645907}, "lstm_clm.clm.Trainer.init": {"tf": 2.449489742783178}, "lstm_clm.clm.Trainer.build": {"tf": 2.6457513110645907}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 2}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.plot_history": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 21}, "s": {"docs": {"lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 2}}, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}}, "df": 1, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "k": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}}, "df": 2, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.fit": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.callbacks": {"tf": 2.23606797749979}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 5}}}}}, "f": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}}, "df": 1}}}}, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 6}}}}}, "c": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}}, "p": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}, "b": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "b": {"docs": {"lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}}, "df": 1}}}}}}, "r": {"docs": {}, "df": 0, "a": {"docs": {"lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.4142135623730951}}, "df": 1}}}}}, "h": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}}, "df": 1}}}}, "m": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}}, "df": 1, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 3, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 3}}}, "m": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 2}}}}}}}}}}}}}}}, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.init": {"tf": 2}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 9, "c": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 3}}}}}}}}}}, "s": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 3}}}}}, "a": {"docs": {}, "df": 0, "x": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 6}, "t": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "b": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 1}}}}}}}}, "p": {"docs": {"lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 8}}}, "t": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.4142135623730951}}, "df": 5, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 6, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 6}}}}}}, "u": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 6}}}, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 2}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}}, "df": 16, "f": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 2}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 2}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 2}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}}, "df": 21}}}}, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 1}}}}}}}}}, "m": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 6}}}, "y": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 2.23606797749979}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 2}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 14}}}}}, "u": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 11}}}}, "o": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}}, "df": 4}, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 1}}}}}}}, "w": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}}, "df": 1}}}, "j": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 2}}, "o": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 4}}}, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 3}}}}, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 3, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1.4142135623730951}}, "df": 1}}}}}, "t": {"docs": {"lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1.4142135623730951}}, "df": 1}}}}}}}, "t": {"docs": {}, "df": 0, "y": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 2.23606797749979}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 2}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 13, "s": {"docs": {"lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 1}}}}}, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 12}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1.4142135623730951}}, "df": 3}}}}}}}}, "m": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 2}}}, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.7320508075688772}}, "df": 4, "f": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}}, "df": 1}}}}}}}}, "s": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 2}}}, "s": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}}, "df": 1}}, "n": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}}, "df": 7, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 2}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 2}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 3.1622776601683795}, "lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 2}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 2}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 34}}}, "u": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 3.605551275463989}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 2}, "lstm_clm.vocab.VocabProto.encode": {"tf": 2.449489742783178}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1.7320508075688772}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1.7320508075688772}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_multi": {"tf": 2.449489742783178}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 2.449489742783178}, "lstm_clm.vocab.encode.encode_data": {"tf": 3.4641016151377544}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.7320508075688772}}, "df": 12}}}, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}}, "df": 1}}}}}}}}}}, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 2.23606797749979}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 2}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 12}}}}}}}, "v": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 6}}, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 11}}}}}}, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "b": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 11, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 6}}}}}, "m": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 2}}}}}}}}}}}}}}}}}}}}}, "s": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}}, "df": 2, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 3}, "f": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}}, "df": 1}}}}}}}}}, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}}, "df": 11}}}, "e": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "f": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}}, "df": 23}}, "q": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 2, "u": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 23}}}}}}}, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "y": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}}, "df": 2}}}}}}}}, "h": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 2}}}}}, "r": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.callbacks": {"tf": 2.23606797749979}, "lstm_clm.clm.Trainer.init": {"tf": 1.7320508075688772}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 10}}, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}}, "df": 1}}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 2}}, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 5}}}}}}}}}}}}}, "r": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 2.23606797749979}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 2}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 2}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_multi": {"tf": 2}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 2}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 2}, "lstm_clm.vocab.encode.encode_data": {"tf": 2.8284271247461903}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 2.449489742783178}}, "df": 30, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1.4142135623730951}}, "df": 3, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}}, "df": 1}}}}}}}}}}}, "o": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}}}}}}, "h": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}}, "df": 1}}}, "u": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 3}}}}}}, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}}, "df": 1}}}}}, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 3}}}}}}}}}, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}}, "df": 1}}}}, "i": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}}, "df": 1, "n": {"docs": {}, "df": 0, "t": {"3": {"2": {"docs": {"lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 10}, "docs": {}, "df": 0}, "docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1.7320508075688772}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 2}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 2}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 2}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 2}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 40}, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}}, "df": 1}}, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.decode.decode_single": {"tf": 1}}, "df": 1}}}}}, "i": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 4}}, "p": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 8}}}}}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}}, "df": 3}}}}}}}}, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}}, "df": 11, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 3}}, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1.4142135623730951}}, "df": 2}}}}}}}}}, "o": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 2}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 13}}, "s": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 4}}, "u": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1.4142135623730951}}, "df": 1}, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}}, "df": 1}}}}}}}}}}}}}}}}}}}}}, "e": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 2}}}}, "p": {"docs": {}, "df": 0, "y": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 2}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 2}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 2}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}}, "df": 21}}}}}, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 3}}, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 2}}}}}, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.init": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 11}}, "b": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 4}}}, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 4}}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 5, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 4}}, "b": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}}}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 3}}}}}, "d": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 5}}}, "f": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "w": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "k": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 2}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}}, "df": 16}}}}}}}}, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"3": {"2": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.7320508075688772}}, "df": 1}, "docs": {}, "df": 0}, "docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 12}}}}, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 13}}}, "c": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1.4142135623730951}}, "df": 1}}}}}}, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}}, "df": 6}}}, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}}, "df": 1}}}}}}, "g": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1.4142135623730951}}, "df": 1}}}}}}, "o": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 2}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 2.23606797749979}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 2.23606797749979}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 2.23606797749979}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}}, "df": 18}, "t": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}}, "df": 1, "i": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1.4142135623730951}}, "df": 2, "s": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}}}}, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}}, "df": 1}}}}}}}, "s": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 4}, "n": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}}, "df": 3}}}}, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 2.23606797749979}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 2}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 12}}, "p": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}}, "df": 1}}}}}, "d": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}}}}}}}, "j": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}}, "df": 1}}}}}}}, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 1}}}, "g": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}}, "df": 2}}}, "s": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 4}}}}}}, "e": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 7, "s": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 3}}}}}, "n": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1.7320508075688772}}, "df": 4}}}}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.vocab.Vocabulary.encode": {"tf": 1}}, "df": 1}}}}}}}}}, "e": {"docs": {"lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 8}}}}}, "x": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}}, "df": 2}}}}}}}}, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.decode.build_decode_map": {"tf": 1}}, "df": 1}}}}}}, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}}}}}}}}}}}, "o": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 4}}}, "k": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.callbacks": {"tf": 2.23606797749979}, "lstm_clm.clm.Trainer.init": {"tf": 1.7320508075688772}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 10}}}}, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}}, "df": 1}}}}, "w": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1}}, "df": 2}}}}}}, "x": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}}, "df": 6}, "h": {"5": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 2}, "docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 2}}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 2, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 1}}}}}}}}}}}}, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}}, "df": 5, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}}, "df": 4}}}}}}}}, "f": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 3}}}}}}}}}}}, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}, "e": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "u": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}}}}}}}}}}}}}}, "p": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 1}}}, "n": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 3}}, "i": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 4}}}}}, "w": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 3}}}}}}}, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "k": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 5}}}}}, "bases": {"root": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}}, "df": 1}}}}}, "s": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}}, "df": 1}}}, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "k": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}}, "df": 1, "s": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}}, "df": 1}}}}}}}}, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.4142135623730951}}, "df": 2}}, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {"lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}}, "df": 1}}}}}}, "l": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}}, "df": 2}}}}, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.MultinomialCLM": {"tf": 1}}, "df": 1, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.MultinomialCLM": {"tf": 1}}, "df": 1}}}}}}}, "m": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}}, "df": 1, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}}, "df": 1}}}}}}}}}}}}}}, "t": {"docs": {"lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}}, "df": 1, "y": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 3}}}}}}, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 3}}}}}}}}, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}}, "df": 1}}}}}}}}}, "doc": {"root": {"0": {"1": {"9": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "3": {"9": {"3": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "docs": {}, "df": 0}, "docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}}, "df": 17}, "1": {"0": {"0": {"0": {"0": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "docs": {}, "df": 0}, "2": {"4": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 4}, "docs": {}, "df": 0}, "docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}}, "df": 1}, "1": {"8": {"6": {"docs": {}, "df": 0, "/": {"docs": {}, "df": 0, "s": {"1": {"3": {"3": {"2": {"1": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "docs": {}, "df": 0}, "docs": {}, "df": 0}, "docs": {}, "df": 0}, "docs": {}, "df": 0}}}, "docs": {}, "df": 0}, "docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}, "2": {"8": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "4": {"0": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 16}, "2": {"0": {"1": {"9": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "docs": {}, "df": 0}, "docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.4142135623730951}, "lstm_clm.vocab": {"tf": 2}}, "df": 6}, "3": {"3": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 1}, "docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}}, "df": 2}, "4": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.vocab": {"tf": 1.4142135623730951}}, "df": 3}, "5": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.vocab": {"tf": 1.7320508075688772}}, "df": 2}, "6": {"docs": {"lstm_clm.vocab": {"tf": 2}}, "df": 1}, "7": {"1": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}, "docs": {"lstm_clm.vocab": {"tf": 1}}, "df": 1}, "docs": {"lstm_clm": {"tf": 3.4641016151377544}, "lstm_clm.callbacks": {"tf": 1.7320508075688772}, "lstm_clm.callbacks.JSDCallback": {"tf": 7.483314773547883}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 6.244997998398398}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 6}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 5.196152422706632}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 6.708203932499369}, "lstm_clm.clm": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM": {"tf": 8.366600265340756}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 7.416198487095663}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 5.656854249492381}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 5.744562646538029}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 5.744562646538029}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 5}, "lstm_clm.clm.MultinomialCLM": {"tf": 6.782329983125268}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 7.14142842854285}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 5.656854249492381}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 7.416198487095663}, "lstm_clm.clm.Trainer": {"tf": 2.23606797749979}, "lstm_clm.clm.Trainer.fit": {"tf": 6.782329983125268}, "lstm_clm.clm.Trainer.callbacks": {"tf": 9.433981132056603}, "lstm_clm.clm.Trainer.init": {"tf": 5.477225575051661}, "lstm_clm.clm.Trainer.build": {"tf": 8.18535277187245}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 6.708203932499369}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1.7320508075688772}, "lstm_clm.clm.allow_memory_growth": {"tf": 1.7320508075688772}, "lstm_clm.clm.batch_tensor_slices": {"tf": 6.48074069840786}, "lstm_clm.clm.build_adam_optimizer": {"tf": 5.0990195135927845}, "lstm_clm.clm.build_model_cp": {"tf": 6}, "lstm_clm.clm.get_data_from_vocab": {"tf": 4.795831523312719}, "lstm_clm.clm.perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 2.449489742783178}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 4.795831523312719}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 5.385164807134504}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 6.855654600401044}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 7.810249675906654}, "lstm_clm.clm.randomized": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 7}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 5.656854249492381}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.dataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.batch_size": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.init_epoch": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.steps": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 15.874507866387544}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 7.14142842854285}, "lstm_clm.clm.randomized.RandomizedDataset.smis": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.vocab": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.rnd_func": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.n_jobs": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.epoch": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.just_shuffle": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.n_precalc": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.verbose": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 6.244997998398398}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 6.855654600401044}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 3.1622776601683795}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 4.58257569495584}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 8}, "lstm_clm.clm.utils": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.NullStrategy": {"tf": 2.8284271247461903}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 4.58257569495584}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 3.3166247903554}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 6.48074069840786}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 5.0990195135927845}, "lstm_clm.clm.utils.build_model_cp": {"tf": 6}, "lstm_clm.clm.utils.categorical_sample": {"tf": 5}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 5.291502622129181}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 5.196152422706632}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 6.557438524302}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 5}, "lstm_clm.clm.utils.plot_history": {"tf": 4.898979485566356}, "lstm_clm.vocab": {"tf": 16.583123951777}, "lstm_clm.vocab.VocabProto": {"tf": 9.38083151964686}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1.7320508075688772}, "lstm_clm.vocab.VocabProto.encode": {"tf": 4.58257569495584}, "lstm_clm.vocab.VocabProto.decode": {"tf": 4.58257569495584}, "lstm_clm.vocab.Vocabulary": {"tf": 9.746794344808963}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 8.717797887081348}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 4.69041575982343}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 4.69041575982343}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.decode_single": {"tf": 5.0990195135927845}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 5.744562646538029}, "lstm_clm.vocab.decode.decode_multi": {"tf": 6.244997998398398}, "lstm_clm.vocab.decode.DecodeFunc": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.DecodeFunc.__init__": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 4.58257569495584}, "lstm_clm.vocab.encode": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.WHITESPACE": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 3.872983346207417}, "lstm_clm.vocab.encode.SplitProto": {"tf": 9.746794344808963}, "lstm_clm.vocab.encode.SplitProto.__init__": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 4.58257569495584}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 7.615773105863909}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 13.674794331177344}, "lstm_clm.vocab.encode.encode_single": {"tf": 5.5677643628300215}, "lstm_clm.vocab.encode.encode_multi": {"tf": 5.477225575051661}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 5.744562646538029}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 6}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 4.358898943540674}, "lstm_clm.vocab.encode.encode_data": {"tf": 7.874007874011811}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 7.3484692283495345}}, "df": 106, "p": {"docs": {"lstm_clm.vocab": {"tf": 1.4142135623730951}}, "df": 1, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm": {"tf": 1}}, "df": 1}}}}}, "t": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.4142135623730951}}, "df": 5}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 2.23606797749979}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.7320508075688772}}, "df": 5, "s": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}}, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 3}}}}}}}}}}}}, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 1}}}, "d": {"docs": {"lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.7320508075688772}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 8, "d": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 7}}}, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}}, "df": 4}}}, "s": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 2}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 5}}}, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 6}}}}}, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 5}}}}}, "b": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}}, "df": 4}, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}}, "df": 6}}}, "y": {"docs": {"lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 1}}}}}}}}, "g": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}}, "df": 2}}}}}}, "e": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}}, "df": 4}, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 6}}}}}}}}, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 4}}}}, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}}, "df": 5, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 3}}}}}, "v": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 1}}}}}}}, "p": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 2}}}}}}, "y": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 2}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.7320508075688772}}, "df": 6}}, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab": {"tf": 1.7320508075688772}}, "df": 1}}}}, "o": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}, "s": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 1, "s": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}}, "df": 1}}}}}}}}, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 8, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.4142135623730951}}, "df": 6}}}}}}}}}, "l": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.utils.NullStrategy": {"tf": 1}}, "df": 1}}}}}}}}}, "o": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 1}}}, "u": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}}, "df": 2}}}}}}, "f": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm": {"tf": 1}, "lstm_clm.callbacks": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 2}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 2}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.7320508075688772}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode": {"tf": 1}, "lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}}, "df": 44}, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}}, "df": 1}}}, "s": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 1}}}}}, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab.decode.build_decode_map": {"tf": 1}}, "df": 1}}}}, "i": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}}, "df": 1}}}, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 3}}, "t": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}}, "df": 2}, "n": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}}, "df": 1}}}}}, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 1}}}, "r": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 4}}}}, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 24}}, "e": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 1, "d": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}}, "df": 1}}}}}}, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 2.449489742783178}}, "df": 7, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 2}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 21, "s": {"docs": {"lstm_clm.clm.utils": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.decode": {"tf": 1}, "lstm_clm.vocab.encode": {"tf": 1}}, "df": 4}}}}}}}}, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"3": {"2": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 2}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 12}, "docs": {}, "df": 0}, "6": {"4": {"docs": {"lstm_clm.clm.utils.categorical_sample": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 2}, "docs": {}, "df": 0}, "docs": {}, "df": 0}}}}, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 2}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 15}}}, "c": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 6}}}}}}, "t": {"3": {"3": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 1}}}, "docs": {}, "df": 0}, "docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1.4142135623730951}}, "df": 5, "h": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback": {"tf": 3.3166247903554}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 2.449489742783178}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 3.605551275463989}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 2}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 2}, "lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 3}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 2.23606797749979}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 3.1622776601683795}, "lstm_clm.clm.Trainer.callbacks": {"tf": 3.4641016151377544}, "lstm_clm.clm.Trainer.init": {"tf": 2}, "lstm_clm.clm.Trainer.build": {"tf": 3}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1.7320508075688772}, "lstm_clm.clm.build_model_cp": {"tf": 2.449489742783178}, "lstm_clm.clm.get_data_from_vocab": {"tf": 2}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 2.23606797749979}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 2.8284271247461903}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 3}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 2.23606797749979}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 2.449489742783178}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 2}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 2}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 3.3166247903554}, "lstm_clm.clm.utils": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.build_model_cp": {"tf": 2.449489742783178}, "lstm_clm.clm.utils.plot_history": {"tf": 1.4142135623730951}, "lstm_clm.vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 2.23606797749979}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 2.23606797749979}, "lstm_clm.vocab.decode": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.encode": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 2.449489742783178}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.7320508075688772}}, "df": 51, "r": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}, "m": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 3}, "y": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 1}}, "a": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1.4142135623730951}}, "df": 2}, "n": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 4}}, "u": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}}, "df": 1}}, "i": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}, "f": {"docs": {"lstm_clm.callbacks": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 19}, "o": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 2.23606797749979}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1.4142135623730951}, "lstm_clm.clm": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 2}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 2.8284271247461903}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 2.449489742783178}, "lstm_clm.clm.Trainer": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 3.1622776601683795}, "lstm_clm.clm.Trainer.init": {"tf": 2}, "lstm_clm.clm.Trainer.build": {"tf": 2.6457513110645907}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 2}, "lstm_clm.clm.build_model_cp": {"tf": 2.23606797749979}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 2.23606797749979}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 2.449489742783178}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 3}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 2.449489742783178}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 2}, "lstm_clm.clm.utils.build_model_cp": {"tf": 2.23606797749979}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto": {"tf": 2}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 2.449489742783178}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 3}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 2}, "lstm_clm.vocab.decode.decode_multi": {"tf": 2}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 2.23606797749979}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_single": {"tf": 2}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 2.23606797749979}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 2.23606797749979}}, "df": 58, "k": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 2}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 2.449489742783178}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 2.23606797749979}}, "df": 18, "s": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto": {"tf": 2.6457513110645907}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 2.6457513110645907}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 2.449489742783178}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 2.23606797749979}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_single": {"tf": 2}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 2.6457513110645907}}, "df": 28}, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 1}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1.7320508075688772}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}}, "df": 6}}}}}, "e": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}}, "df": 2, "d": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}}, "df": 2}}}}}}}, "d": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 2}}}}}}, "o": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 1}}, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.Trainer": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.7320508075688772}}, "df": 8, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 14, "c": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}}}}}}}, "u": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 2}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 2.23606797749979}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 2}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 12}}, "i": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab": {"tf": 1}}, "df": 1}}}}}}}, "u": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 8, "[": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "f": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 3}}}}}}}, "e": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm": {"tf": 1}}, "df": 1}}, "n": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}}, "df": 10, "f": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}}, "df": 3}}}}, "s": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 2}, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.utils.init_tensor_array": {"tf": 1.4142135623730951}}, "df": 1}}}}}}}}}, "m": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 6, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 6}}}}}}}}}}, "i": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 6, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 1}}}}, "q": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 2}}}, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1.4142135623730951}}, "df": 2}}}, "k": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}}, "df": 2}}}}, "y": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 1, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 1}}}}}}}}, "w": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}}, "df": 2}}}, "c": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}}, "df": 2, "l": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1.4142135623730951}}, "df": 2, "m": {"docs": {"lstm_clm": {"tf": 1.4142135623730951}, "lstm_clm.clm": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity": {"tf": 1}, "lstm_clm.clm.utils": {"tf": 1}, "lstm_clm.vocab": {"tf": 1.4142135623730951}}, "df": 7}, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.Trainer": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 4, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm": {"tf": 1}}, "df": 1}}}}}}, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 4, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "k": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 2}, "lstm_clm.clm.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.4142135623730951}}, "df": 3, "s": {"docs": {"lstm_clm": {"tf": 1}, "lstm_clm.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.7320508075688772}}, "df": 4}}}}}, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 2}}}, "c": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}}, "df": 2, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 5, "t": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}}, "df": 1}, "d": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}}, "df": 3}, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 2}}, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 3}}}}}}}}}, "n": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}}, "df": 3, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}}, "df": 2}}}}}}}, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 1}}}}}, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.callbacks": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 2}}}}, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}}, "df": 3}}}}}, "t": {"docs": {"lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 1, "s": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}}, "df": 1}, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 2}}}}}}, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}}, "df": 1}}}}}}, "i": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 3}}}, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 1}}}}}}}, "/": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "/": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}}}}}}}}}}}}}}}}, "n": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.Trainer": {"tf": 1}}, "df": 1}}}}}}}}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}}, "df": 1}}}, "a": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 1}}}}}}}, "h": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}, "c": {"docs": {}, "df": 0, "k": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.4142135623730951}}, "df": 3}}}}}}}}, "a": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}}, "df": 2, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 2.23606797749979}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 2.23606797749979}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 11, "s": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}}, "df": 4}}}}}}}}}, "e": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}}, "df": 1}}}, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}}, "df": 1}}}, "e": {"docs": {"lstm_clm.vocab": {"tf": 1}}, "df": 1, "s": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}}, "df": 2}}}}}}, "b": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}}, "df": 1}}, "p": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}, "c": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}}, "df": 1}, "c": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1.7320508075688772}}, "df": 2, "c": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}}, "df": 1, "l": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 1}}}}}}}}, "m": {"1": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}, "docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 2, "l": {"docs": {"lstm_clm": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 2}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 2}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.init": {"tf": 2}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 2}, "lstm_clm.clm.build_model_cp": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 2}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 26, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_model_cp": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.7320508075688772}}, "df": 6}}}}, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}, "c": {"docs": {}, "df": 0, "k": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}}, "df": 1}}, "r": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 1}}}, "e": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}}, "df": 1, "s": {"docs": {"lstm_clm.callbacks": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 2}}}}, "h": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 2, "s": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 2}}}}}, "m": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 2}}}}, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 1}}}}, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 3}}, "l": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}}, "df": 2, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 3, "l": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}}, "df": 1}}}}}}}}, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}}, "df": 2}}}}}}}, "a": {"docs": {}, "df": 0, "x": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 10, "i": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 9}}}}}, "t": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}}, "df": 1}}}, "k": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}, "n": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}}, "df": 1}}}}}, "p": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 2}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 10, "p": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 10}}}}}, "y": {"docs": {"lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}}, "df": 2}, "i": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 1}}}}, "u": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm": {"tf": 1}, "lstm_clm.clm.randomized": {"tf": 1}}, "df": 2}}}, "y": {"docs": {"lstm_clm.clm.utils": {"tf": 1}}, "df": 1}}}}}}, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}}, "df": 15, "d": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}}, "df": 5}, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}}, "df": 3}}, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.clm.randomized": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 7}}}, "a": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}}, "df": 2}}}}, "n": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}}, "df": 2}}}}, "i": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}, "q": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}}, "df": 2}}}}, "k": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 7, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 8}}}}}}}, "i": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}}, "df": 1, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab": {"tf": 1.7320508075688772}}, "df": 1, "s": {"docs": {"lstm_clm": {"tf": 1}}, "df": 1}, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 2}}}}}}, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}}, "df": 1, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1.4142135623730951}}, "df": 2}}}}}, "s": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}}, "df": 1}}}}}}}}, "a": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 1}}}}, "n": {"docs": {"lstm_clm": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 20, "t": {"3": {"2": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}}, "df": 19}, "docs": {}, "df": 0}, "docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 2}, "lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}}, "df": 10, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}}, "df": 3, "s": {"docs": {"lstm_clm.vocab.decode.decode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 4}}}}}, "o": {"docs": {"lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 2}, "s": {"docs": {"lstm_clm.vocab.encode.encode_single": {"tf": 1}}, "df": 1}}, "i": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 5, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 8, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}}, "df": 7, "s": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}}, "df": 1}, "d": {"docs": {"lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 3}}}}}}}}}, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 2}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 5}}, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_single": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}}, "df": 11}}}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}}, "df": 4}}}}, "e": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 2}}}}}, "p": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 2}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}}, "df": 10, "s": {"docs": {"lstm_clm.vocab.VocabProto.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 8}}}}, "f": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 2}, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 1}}}}}}, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 1}}}}}}, "s": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 2.23606797749979}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.7320508075688772}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 2}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 2.23606797749979}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 22, "s": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}}}, "f": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 2}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 2.6457513110645907}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 3}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 2}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.7320508075688772}}, "df": 27}, "t": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 3, "s": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}}, "df": 1}, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}}, "df": 3}}}}}}}}, "a": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 2}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 2}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 2}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto": {"tf": 1.7320508075688772}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.decode_single": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 2}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 2.449489742783178}}, "df": 48, "r": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm": {"tf": 1}, "lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 8}, "g": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 59}}}}}}}, "\u00fa": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.vocab.encode.encode_multi": {"tf": 1}}, "df": 1}}}}, "n": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 9, "d": {"docs": {"lstm_clm": {"tf": 1}, "lstm_clm.callbacks.JSDCallback": {"tf": 1.7320508075688772}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 2.23606797749979}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.init": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1.7320508075688772}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.7320508075688772}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 3}}, "df": 27, "/": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 1}}}}, "y": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}}, "df": 3}}, "s": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}}, "df": 9, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 7}}}, "i": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}}, "df": 1}}}}}}, "t": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 6, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}}, "df": 1, "s": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 8}}}}}}}}}, "f": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 2.23606797749979}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 3}}}}, "c": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 1}}}}}}}}, "l": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1, "l": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}}, "df": 2, "o": {"docs": {}, "df": 0, "w": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 2}}}, "w": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 2}}}}, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}}, "df": 1}}}}}}, "b": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}}, "df": 1}}}}}}, "c": {"docs": {"lstm_clm.vocab": {"tf": 1.4142135623730951}}, "df": 1}}, "p": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}}, "df": 2}}}, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}}, "d": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 2, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.7320508075688772}, "lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 3}}}}}}}, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 2}}}, "a": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1.7320508075688772}}, "df": 2}}, "j": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.decode.build_decode_map": {"tf": 1}}, "df": 1}, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}}, "df": 2}}, "m": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 1}}}}}}}}}}}, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 3, "u": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm": {"tf": 1}}, "df": 1}}}}}}}}}, "h": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1.4142135623730951}}, "df": 4}}}}, "p": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.4142135623730951}, "lstm_clm.vocab": {"tf": 1}}, "df": 2}}}, "o": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 4}}}, "w": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}}, "df": 2}}, "u": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}}, "df": 6, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 3}}}}}}, "i": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}}, "df": 1}}}}}}, "a": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 7, "s": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 2}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 2}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}}, "df": 12}, "d": {"docs": {"lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 3}}, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}}, "df": 2}}}}}, "e": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}, "v": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 2}}}, "e": {"docs": {"lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 2}, "lstm_clm.clm.utils.build_model_cp": {"tf": 2}}, "df": 3}}}, "e": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1.4142135623730951}}, "df": 3, "t": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}}, "df": 5, "s": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.7320508075688772}}, "df": 2}, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.get_data_from_vocab": {"tf": 1}}, "df": 1}}}}}}, "q": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}}, "df": 5, "u": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}}, "df": 10}}}}}}, "n": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 3}}}}}}, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 1}}}}}}}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 2}}}}}, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}}, "df": 1}}}}}}}, "r": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 10}}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 2}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2}}, "df": 3}}, "i": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1.4142135623730951}}, "df": 2}}}}, "r": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 2.23606797749979}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 2.23606797749979}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 11, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 9, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 2}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}}, "df": 16}}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.init": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1.4142135623730951}}, "df": 5}}}}}}, "e": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 6, "s": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}}, "df": 2}}}, "o": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 1, "p": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1.7320508075688772}}, "df": 1}}}}}, "r": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1, "r": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1, "/": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}}, "df": 1}}}}}}, "d": {"docs": {"lstm_clm.clm.get_data_from_vocab": {"tf": 1}}, "df": 1}}}}}, "i": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1.4142135623730951}}, "df": 1, "z": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 2}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 2}, "lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1.4142135623730951}}, "df": 16, "d": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 1}}}, "n": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 6}}}}}, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 2.23606797749979}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 2}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 13}}}, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 2.23606797749979}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}}, "df": 2}}}, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}}, "df": 7}}, "f": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}}}}}, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 2}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 2}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 6, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 1}}}, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1.7320508075688772}}, "df": 1}}}}}}}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 5}}}}, "h": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.4142135623730951}}, "df": 2}}}}}, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 2}}}}}, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}}, "v": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "b": {"docs": {"lstm_clm": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 2}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.vocab": {"tf": 2}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}}, "df": 25, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 2}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 2.23606797749979}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.7320508075688772}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.vocab": {"tf": 2.23606797749979}, "lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}}, "df": 26}}}}}, "m": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 1}}}, "m": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 1}}}}}}}}}}}}}}}}, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 2}}}}}}}}}, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.7320508075688772}}, "df": 7, "i": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 8}}}}}}}, "u": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}}, "df": 2}, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 8}}}}}}}}, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}}, "df": 1}}}}}}}}, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 13}, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 11}}}}}}, "s": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}}}}}, "j": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1.4142135623730951}}, "df": 1, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1.4142135623730951}}, "df": 4}}}}}, "s": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.7320508075688772}}, "df": 2, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "k": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}}, "df": 1}}}}}}}}}}, "o": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}, "b": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 2}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}}, "df": 5}}, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab.decode.decode_single": {"tf": 1}}, "df": 1}}}}}, "u": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 7}}}}, "d": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 3, "i": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1.7320508075688772}}, "df": 4}}}}}}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.utils.multinomial_sample": {"tf": 1.4142135623730951}}, "df": 1, "s": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}, "e": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1}}, "df": 2, "d": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}}, "df": 2, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 1}}}}}}}}}}}}}}}}, "f": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 2}}}}}}}, "m": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 5}}, "c": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 1}}}}}}, "[": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 2}}}, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 2}}}}}}, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 1, "s": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 1}}}}}}}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 2.23606797749979}, "lstm_clm.clm.batch_tensor_slices": {"tf": 2.23606797749979}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 2.23606797749979}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1.4142135623730951}}, "df": 10, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.7320508075688772}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1.4142135623730951}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1.4142135623730951}}, "df": 13, "p": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}}, "df": 1}}}}}, "s": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}}, "df": 2}}}}}}}, "e": {"docs": {}, "df": 0, "f": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}}, "df": 1, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.__init__": {"tf": 2.23606797749979}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 2.23606797749979}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 2.23606797749979}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 2.449489742783178}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 2.23606797749979}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_model_cp": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 2}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 2}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 2.449489742783178}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 30}}}}}, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}}, "df": 2}}}}}, "c": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.init": {"tf": 1}}, "df": 2}}, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto": {"tf": 1.7320508075688772}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.decode_multi": {"tf": 2}, "lstm_clm.vocab.decode.DecodeFunc": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 2.6457513110645907}}, "df": 11, "d": {"docs": {"lstm_clm.vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}}, "df": 5}}, "a": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}}, "df": 1}}}}}}}, "v": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 2}}}}}, "s": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 2, "r": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 2}}}}}}}}}}, "o": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}}, "df": 1}}, "n": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 1}}}, "s": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}}, "df": 1}, "t": {"docs": {}, "df": 0, "y": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.utils.init_tensor_array": {"tf": 1.4142135623730951}}, "df": 1}}}}}, "b": {"docs": {"lstm_clm.vocab": {"tf": 1.7320508075688772}}, "df": 1, "e": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}}, "df": 9, "t": {"docs": {}, "df": 0, "w": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}}, "df": 2}}}}}, "f": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 3}}}}, "g": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 6}}}}}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 3}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 2.23606797749979}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 2}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 2}, "lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.batch_tensor_slices": {"tf": 2}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 2.23606797749979}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 2.23606797749979}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 2}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2.23606797749979}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 28, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 2.23606797749979}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 2}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 4, "*": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}}, "df": 1}}}}}}}, "d": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 3}}}}}, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}}, "df": 2, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1.4142135623730951}}, "df": 2}}}}}, "r": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}}, "df": 2}}}, "o": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 2}}, "df": 5}}, "s": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 2.449489742783178}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 9}}, "u": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 2}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 2.449489742783178}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 10, "s": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 3}}, "t": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}}, "df": 2}}}}, "c": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab": {"tf": 1.4142135623730951}}, "df": 1}}, "r": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 2, "|": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "|": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "i": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 1}}}}}}}}}}}}}}}, "c": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 1}}}}}}, "y": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 3}}, "n": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 2.23606797749979}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.7320508075688772}}, "df": 9, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.4142135623730951}}, "df": 2, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}}, "df": 1}}}, "o": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 3, "t": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}}, "df": 10, "e": {"docs": {"lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 1}}, "n": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 2, "e": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 2}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.7320508075688772}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 26}}, "r": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 1, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}}, "df": 1}}}}}}}}}}}}, "u": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 20}}}}, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}}, "df": 2, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1}}, "df": 1}}}}}}}}}}, "e": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 5}}}}}, "x": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}}, "df": 3}}, "e": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 2}}}, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 1}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab.decode.decode_multi": {"tf": 1}}, "df": 1}}}}}, "p": {"docs": {"lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}}, "df": 2}, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.vocab": {"tf": 1}}, "df": 1, "/": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "f": {"docs": {"lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}}, "df": 2}}}}}}}}}}, "o": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}}, "df": 2, "f": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1.4142135623730951}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1.7320508075688772}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 2}, "lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 2.8284271247461903}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 2.23606797749979}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 2.6457513110645907}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 2}}, "df": 51, "f": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 2}}, "r": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 9, "g": {"docs": {}, "df": 0, "/": {"1": {"0": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}, "docs": {}, "df": 0}, "docs": {}, "df": 0}}}, "n": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}}, "df": 5, "e": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 3}, "l": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 2}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 2}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}}, "df": 7}}}, "u": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.7320508075688772}}, "df": 4}}}}}, "p": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}}, "df": 1, "i": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1.7320508075688772}}, "df": 5}}}}}, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 2}}, "s": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 1}}}}}}, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 3}}}, "b": {"docs": {}, "df": 0, "j": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.utils.plot_history": {"tf": 1.4142135623730951}}, "df": 1}}}}}, "t": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 2}}}}}, "w": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.7320508075688772}, "lstm_clm.clm.VocabMultinomialCLM.ASSERT_VOCAB": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 27}}, "l": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 1}}, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 3}}}, "h": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 11}}}}}, "i": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 2}}, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 2}}}}}}}}}, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}}, "df": 5, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}}, "df": 7}}}}}}, "e": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 3, "i": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1.4142135623730951}}, "df": 3}}}}}}, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 1}}}}}, "g": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 3, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 6}, "n": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 3, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.utils": {"tf": 1}}, "df": 4, "w": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}}, "df": 1}}}}}}}}}, "i": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}, "o": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}}, "df": 5}}, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}}, "df": 2}}}, "e": {"docs": {"lstm_clm.clm": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.7320508075688772}}, "df": 4, "d": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}}, "df": 7}}}}}}, "m": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}}, "df": 1}}}}}}}, "p": {"docs": {}, "df": 0, "u": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 4, "s": {"docs": {"lstm_clm.clm.Trainer.init": {"tf": 1}}, "df": 1}}}, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.allow_memory_growth": {"tf": 1}, "lstm_clm.clm.utils.allow_memory_growth": {"tf": 1}}, "df": 2}}}}}, "i": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 3}}}}, "t": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 3.872983346207417}, "lstm_clm.vocab": {"tf": 6}, "lstm_clm.vocab.encode.SplitProto": {"tf": 3}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 3}}, "df": 4}}, "l": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.callbacks.JSDCallback": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}}, "df": 11}}}, "n": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 15, "g": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1.7320508075688772}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 2.23606797749979}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 27, "s": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}}, "df": 1}, "[": {"1": {"docs": {"lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 2}, "docs": {}, "df": 0}}}}}, "a": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1.7320508075688772}}, "df": 2}}}}}}, "g": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2}}}}, "f": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 5}}}, "o": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1.7320508075688772}}, "df": 1}, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.utils.categorical_sample": {"tf": 1.7320508075688772}}, "df": 1}}}}, "a": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 2, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1}, "lstm_clm.clm.BaseCLM.save_optim": {"tf": 1}}, "df": 2}}}}}, "s": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.load_optim": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 2}}, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 1, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}}, "df": 4}}}}}, "i": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 2}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1.4142135623730951}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1.7320508075688772}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 2.449489742783178}}, "df": 19, "[": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}}, "df": 3}}}, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "[": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "f": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 2}}}}}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}}, "df": 3}}}}, "/": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.vocab.Vocabulary.encode": {"tf": 1}}, "df": 1}}}, "s": {"docs": {"lstm_clm.vocab.Vocabulary.decode": {"tf": 1}}, "df": 1}}}, "k": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 1}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm": {"tf": 1}, "lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.perplexity": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2.23606797749979}, "lstm_clm.vocab": {"tf": 1.4142135623730951}}, "df": 11}}}, "a": {"docs": {}, "df": 0, "y": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 5, "s": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2.23606797749979}}, "df": 7}}}}, "m": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "a": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}}, "df": 1}}}}, "r": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 1}}}}}}, "r": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 1, "e": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 2, "t": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}}, "df": 2, "s": {"docs": {"lstm_clm.callbacks.JSDCallback.get_jsd": {"tf": 1}, "lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}, "lstm_clm.clm.BaseCLM.from_vocab": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1}, "lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.NullStrategy.scope": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.categorical_sample": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.init_tensor_array": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.VocabProto.__len__": {"tf": 1}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.VocabProto.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.decode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__len__": {"tf": 1}, "lstm_clm.vocab.decode.decode_single": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 2}}, "df": 53}}}}}, "q": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.perplexity.generate_with_perplexity": {"tf": 1}}, "df": 1}}}}}}, "p": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 1}}, "g": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "x": {"docs": {"lstm_clm.vocab.encode.WHITESPACE": {"tf": 1}, "lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 5}}}}, "a": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "z": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 4, "d": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 7, "d": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.Trainer": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}}, "df": 5}}}}}}}}, "f": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "c": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}}, "df": 1}}}}, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}}, "df": 2}}, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 3}}}, "a": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}}, "df": 1}}}}}}}}}}}}, "i": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}}, "df": 1, "s": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 13}, "d": {"docs": {"lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 1}}}}, "t": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.build_adam_optimizer": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.build_adam_optimizer": {"tf": 1.7320508075688772}}, "df": 2}}}, "n": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.Trainer.build": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 4}}, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}}, "df": 1, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.7320508075688772}}, "df": 1}}, "i": {"docs": {}, "df": 0, "g": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 5}}}}}, "e": {"docs": {"lstm_clm.vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 4, "v": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1.4142135623730951}}, "df": 3, "t": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1}}, "df": 1}}}}}}}}}, "p": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.callbacks.JSDCallback.on_epoch_end": {"tf": 1.7320508075688772}, "lstm_clm.clm.Trainer.init": {"tf": 1}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 2}, "lstm_clm.clm.randomized.BuiltRandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1.7320508075688772}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 14, "s": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1.4142135623730951}, "lstm_clm.clm.Trainer.build": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.used": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.build": {"tf": 1}}, "df": 7}}}}}, "t": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}, "m": {"docs": {}, "df": 0, "b": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.7320508075688772}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}}, "df": 5}}}}}}}, "p": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}}, "df": 3}}}}, "l": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.BaseCLM.__init__": {"tf": 1}, "lstm_clm.clm.BaseCLM.call_cell": {"tf": 1}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 11}}, "e": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1.4142135623730951}}, "df": 1}}}}}}, "o": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1.4142135623730951}, "lstm_clm.clm.get_data_from_vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 2.449489742783178}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 9}}, "x": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.clm.BaseCLM.__init__": {"tf": 1}}, "df": 1}}}}}}, "e": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "d": {"docs": {"lstm_clm.vocab": {"tf": 1}}, "df": 1}}}}}}, "c": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}}, "df": 1}}}, "l": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}}, "df": 2, "d": {"docs": {"lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.decode.decode_multi": {"tf": 1}, "lstm_clm.vocab.decode.build_full_decode_func": {"tf": 1}}, "df": 3}}}}}}, "a": {"docs": {}, "df": 0, "m": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 2}}}}}}, "a": {"docs": {}, "df": 0, "c": {"docs": {}, "df": 0, "h": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1.4142135623730951}, "lstm_clm.clm.randomized.RandomizedDataset.randomized_perplexity": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 4}}, "r": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1.7320508075688772}}, "df": 1}}}}, "q": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {"lstm_clm.clm.MultinomialCLM.generate_with_perplexity": {"tf": 1}, "lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.perplexity_from_samples": {"tf": 1}}, "df": 3}}}}, "n": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "m": {"docs": {"lstm_clm.clm.Trainer.callbacks": {"tf": 1}}, "df": 1}}, "c": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.__init__": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}}, "df": 6}}}, "e": {"docs": {"lstm_clm.vocab": {"tf": 1.4142135623730951}, "lstm_clm.vocab.VocabProto": {"tf": 1.7320508075688772}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.decode.build_decode_map": {"tf": 1}, "lstm_clm.vocab.encode": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1.7320508075688772}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_encode_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 12, "d": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.build_dataset": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.encoded": {"tf": 1}, "lstm_clm.vocab": {"tf": 2}, "lstm_clm.vocab.VocabProto.encode": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.encode_multi": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1.4142135623730951}}, "df": 8}, "s": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset.on_epoch_begin": {"tf": 1}, "lstm_clm.clm.randomized.RandomizedDataset.on_train_begin": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 3}}}}, "l": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.vocab.encode.tokenize_string": {"tf": 1}}, "df": 1}}}}}}, "d": {"docs": {"lstm_clm.vocab": {"tf": 1}, "lstm_clm.vocab.VocabProto": {"tf": 1}, "lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 1}, "lstm_clm.vocab.encode.build_tokenize_func": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1.4142135623730951}}, "df": 8}}, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.clm.batch_tensor_slices": {"tf": 1}, "lstm_clm.clm.utils.batch_tensor_slices": {"tf": 1}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}}, "df": 3}}}}}, "r": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.pad_tokens": {"tf": 1}, "lstm_clm.vocab.encode.encode_single": {"tf": 1}, "lstm_clm.vocab.encode.build_pad_func": {"tf": 1}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}, "lstm_clm.vocab.encode.build_full_encode_func": {"tf": 1}}, "df": 6}}}}}, "q": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}, "o": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.clm.randomized.RandomizedDataset": {"tf": 3.4641016151377544}, "lstm_clm.vocab": {"tf": 5.291502622129181}, "lstm_clm.vocab.encode.SplitProto": {"tf": 4.358898943540674}, "lstm_clm.vocab.encode.tokenize_string": {"tf": 4.47213595499958}}, "df": 4}}}}, "h": {"5": {"docs": {"lstm_clm.clm.build_model_cp": {"tf": 1}, "lstm_clm.clm.utils.build_model_cp": {"tf": 1}}, "df": 2}, "docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "p": {"docs": {}, "df": 0, "s": {"docs": {}, "df": 0, ":": {"docs": {}, "df": 0, "/": {"docs": {}, "df": 0, "/": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "i": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}, "g": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "h": {"docs": {}, "df": 0, "u": {"docs": {}, "df": 0, "b": {"docs": {"lstm_clm.callbacks.jensen_shannon_divergence": {"tf": 1}}, "df": 1}}}}}}}}}}}}}, "a": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.MultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1.4142135623730951}, "lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 4, "h": {"docs": {"lstm_clm.clm.perplexity.build_default_prior_probs": {"tf": 1}, "lstm_clm.clm.perplexity.build_hash_table": {"tf": 1.4142135623730951}}, "df": 2}}, "v": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}}, "df": 1}}}, "i": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 2}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 2}}, "df": 3}}}}, "s": {"docs": {}, "df": 0, "t": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "y": {"docs": {"lstm_clm.clm.Trainer.fit": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1.7320508075688772}}, "df": 2}}}}}}, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "e": {"docs": {"lstm_clm.vocab.encode.encode_data": {"tf": 1}}, "df": 1}}}}, "k": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "a": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.BaseCLM": {"tf": 1}, "lstm_clm.clm.MultinomialCLM": {"tf": 1}, "lstm_clm.clm.VocabMultinomialCLM": {"tf": 1}, "lstm_clm.clm.utils.plot_history": {"tf": 1}}, "df": 4}}}, "e": {"docs": {}, "df": 0, "p": {"docs": {"lstm_clm.vocab.encode.REGEX_PATTERNS": {"tf": 1.4142135623730951}}, "df": 1}}, "p": {"docs": {}, "df": 0, "t": {"docs": {"lstm_clm.vocab.encode.SplitProto": {"tf": 1}}, "df": 1}}}, "n": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "w": {"docs": {}, "df": 0, "n": {"docs": {"lstm_clm.vocab.Vocabulary": {"tf": 1}, "lstm_clm.vocab.Vocabulary.__init__": {"tf": 1.4142135623730951}}, "df": 2}}}}}, "x": {"docs": {"lstm_clm.clm.BaseCLM.call_cell": {"tf": 1.7320508075688772}, "lstm_clm.clm.MultinomialCLM.sample_next_tokens": {"tf": 1}, "lstm_clm.clm.utils.NullStrategy.experimental_distribute_dataset": {"tf": 1}, "lstm_clm.clm.utils.init_hidden_states": {"tf": 1}, "lstm_clm.clm.utils.lstm_model_call": {"tf": 1.7320508075688772}, "lstm_clm.clm.utils.multinomial_sample": {"tf": 1}, "lstm_clm.vocab.Vocabulary.encode": {"tf": 1}, "lstm_clm.vocab.encode.SplitProto.split": {"tf": 1}}, "df": 8}, "y": {"docs": {"lstm_clm.clm.perplexity.calc_perplexity": {"tf": 1}}, "df": 1, "i": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "l": {"docs": {}, "df": 0, "d": {"docs": {}, "df": 0, "i": {"docs": {}, "df": 0, "n": {"docs": {}, "df": 0, "g": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 1}}}, "s": {"docs": {"lstm_clm.clm.randomized.BuiltRandomizedDataset": {"tf": 1}}, "df": 1}}}}}}, "z": {"docs": {}, "df": 0, "e": {"docs": {}, "df": 0, "r": {"docs": {}, "df": 0, "o": {"docs": {}, "df": 0, "s": {"docs": {"lstm_clm.clm.utils.init_hidden_states": {"tf": 1}}, "df": 1}}}}}}}}, "pipeline": ["trimmer"], "_isPrebuiltIndex": true};

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();
